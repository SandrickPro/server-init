#!/usr/bin/env python3
"""
Server Init - Iteration 92: Log Aggregation Platform
–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ª–æ–≥–æ–≤

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- Log Collection - —Å–±–æ—Ä –ª–æ–≥–æ–≤
- Log Parsing - –ø–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–æ–≤
- Log Storage - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
- Log Search - –ø–æ–∏—Å–∫ –ø–æ –ª–æ–≥–∞–º
- Log Analysis - –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
- Pattern Detection - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- Alert Rules - –ø—Ä–∞–≤–∏–ª–∞ –∞–ª–µ—Ä—Ç–æ–≤
- Retention Management - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Ç–µ–Ω—à–µ–Ω–æ–º
"""

import json
import asyncio
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Set, Callable, Pattern, Tuple
from enum import Enum
from collections import defaultdict
import uuid
import random
import re


class LogLevel(Enum):
    """–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∞"""
    TRACE = 0
    DEBUG = 1
    INFO = 2
    WARN = 3
    ERROR = 4
    FATAL = 5


class LogSource(Enum):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –ª–æ–≥–æ–≤"""
    APPLICATION = "application"
    SYSTEM = "system"
    SECURITY = "security"
    NETWORK = "network"
    DATABASE = "database"


class AlertState(Enum):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞"""
    PENDING = "pending"
    FIRING = "firing"
    RESOLVED = "resolved"


@dataclass
class LogEntry:
    """–ó–∞–ø–∏—Å—å –ª–æ–≥–∞"""
    log_id: str
    
    # –í—Ä–µ–º—è
    timestamp: datetime = field(default_factory=datetime.now)
    
    # –£—Ä–æ–≤–µ–Ω—å
    level: LogLevel = LogLevel.INFO
    
    # –°–æ–æ–±—â–µ–Ω–∏–µ
    message: str = ""
    
    # –ò—Å—Ç–æ—á–Ω–∏–∫
    source: LogSource = LogSource.APPLICATION
    service: str = ""
    host: str = ""
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç
    trace_id: str = ""
    span_id: str = ""
    request_id: str = ""
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    fields: Dict[str, Any] = field(default_factory=dict)
    
    # –¢–µ–≥–∏
    tags: List[str] = field(default_factory=list)
    
    # Raw –ª–æ–≥
    raw: str = ""
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    parsed: bool = False
    parser_name: str = ""


@dataclass
class LogStream:
    """–ü–æ—Ç–æ–∫ –ª–æ–≥–æ–≤"""
    stream_id: str
    name: str = ""
    
    # –§–∏–ª—å—Ç—Ä—ã
    services: List[str] = field(default_factory=list)
    hosts: List[str] = field(default_factory=list)
    levels: List[LogLevel] = field(default_factory=list)
    
    # –ë—É—Ñ–µ—Ä
    buffer: List[LogEntry] = field(default_factory=list)
    buffer_size: int = 1000
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_logs: int = 0
    logs_per_second: float = 0


@dataclass
class LogPattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –ª–æ–≥–∞"""
    pattern_id: str
    name: str = ""
    
    # Regex –ø–∞—Ç—Ç–µ—Ä–Ω
    regex: str = ""
    compiled: Optional[Pattern] = None
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º—ã–µ –ø–æ–ª—è
    fields: List[str] = field(default_factory=list)
    
    # –ü—Ä–∏–º–µ—Ä—ã
    examples: List[str] = field(default_factory=list)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    match_count: int = 0


@dataclass
class LogQuery:
    """–ó–∞–ø—Ä–æ—Å –∫ –ª–æ–≥–∞–º"""
    query_id: str
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    text: str = ""
    regex: str = ""
    
    # –§–∏–ª—å—Ç—Ä—ã
    services: List[str] = field(default_factory=list)
    hosts: List[str] = field(default_factory=list)
    levels: List[LogLevel] = field(default_factory=list)
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    # –ü–æ–ª—è
    field_filters: Dict[str, Any] = field(default_factory=dict)
    
    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    limit: int = 100
    offset: int = 0


@dataclass
class LogQueryResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞"""
    query_id: str
    total_count: int = 0
    returned_count: int = 0
    
    logs: List[LogEntry] = field(default_factory=list)
    
    # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    execution_time_ms: float = 0
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏
    aggregations: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LogAlert:
    """–ê–ª–µ—Ä—Ç –ø–æ –ª–æ–≥–∞–º"""
    alert_id: str
    name: str = ""
    description: str = ""
    
    # –£—Å–ª–æ–≤–∏–µ
    query: str = ""
    threshold: int = 1
    time_window_minutes: int = 5
    
    # –£—Ä–æ–≤–µ–Ω—å
    severity: str = "warning"
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ
    state: AlertState = AlertState.PENDING
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
    current_count: int = 0
    
    # –í—Ä–µ–º—è
    last_triggered: Optional[datetime] = None
    
    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
    notification_channels: List[str] = field(default_factory=list)


@dataclass
class RetentionPolicy:
    """–ü–æ–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
    policy_id: str
    name: str = ""
    
    # –°—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è
    retention_days: int = 30
    
    # –§–∏–ª—å—Ç—Ä—ã
    services: List[str] = field(default_factory=list)
    levels: List[LogLevel] = field(default_factory=list)
    
    # –î–µ–π—Å—Ç–≤–∏—è
    archive_before_delete: bool = True
    compress: bool = True


class LogParser:
    """–ü–∞—Ä—Å–µ—Ä –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.patterns: Dict[str, LogPattern] = {}
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self._add_default_patterns()
        
    def _add_default_patterns(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        # Apache Combined Log Format
        self.add_pattern(
            "apache_combined",
            r'^(?P<ip>\S+) \S+ \S+ \[(?P<timestamp>[^\]]+)\] "(?P<method>\S+) (?P<path>\S+) \S+" (?P<status>\d+) (?P<size>\d+)',
            ["ip", "timestamp", "method", "path", "status", "size"]
        )
        
        # Nginx
        self.add_pattern(
            "nginx",
            r'^(?P<ip>\S+) - \S+ \[(?P<timestamp>[^\]]+)\] "(?P<request>[^"]+)" (?P<status>\d+) (?P<bytes>\d+)',
            ["ip", "timestamp", "request", "status", "bytes"]
        )
        
        # JSON
        self.add_pattern(
            "json",
            r'^\{.*\}$',
            []
        )
        
        # Syslog
        self.add_pattern(
            "syslog",
            r'^(?P<priority><\d+>)?(?P<timestamp>\w{3}\s+\d+\s+\d+:\d+:\d+) (?P<host>\S+) (?P<process>\S+): (?P<message>.*)$',
            ["priority", "timestamp", "host", "process", "message"]
        )
        
    def add_pattern(self, name: str, regex: str, fields: List[str]) -> LogPattern:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        pattern = LogPattern(
            pattern_id=f"pattern_{uuid.uuid4().hex[:8]}",
            name=name,
            regex=regex,
            compiled=re.compile(regex),
            fields=fields
        )
        self.patterns[name] = pattern
        return pattern
        
    def parse(self, raw: str, pattern_name: str = None) -> Tuple[bool, Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–∞"""
        if pattern_name and pattern_name in self.patterns:
            return self._try_pattern(raw, self.patterns[pattern_name])
            
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in self.patterns.values():
            success, fields = self._try_pattern(raw, pattern)
            if success:
                return success, fields
                
        return False, {}
        
    def _try_pattern(self, raw: str, pattern: LogPattern) -> Tuple[bool, Dict[str, Any]]:
        """–ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω"""
        if pattern.name == "json":
            try:
                data = json.loads(raw)
                pattern.match_count += 1
                return True, data
            except:
                return False, {}
                
        if pattern.compiled:
            match = pattern.compiled.match(raw)
            if match:
                pattern.match_count += 1
                return True, match.groupdict()
                
        return False, {}


class LogStorage:
    """–•—Ä–∞–Ω–∏–ª–∏—â–µ –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.logs: Dict[str, LogEntry] = {}
        self.indexes: Dict[str, Dict[Any, List[str]]] = defaultdict(lambda: defaultdict(list))
        
    def store(self, log: LogEntry):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞"""
        self.logs[log.log_id] = log
        
        # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        self.indexes["service"][log.service].append(log.log_id)
        self.indexes["host"][log.host].append(log.log_id)
        self.indexes["level"][log.level].append(log.log_id)
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å (–ø–æ —á–∞—Å–∞–º)
        hour_key = log.timestamp.strftime("%Y-%m-%d-%H")
        self.indexes["time"][hour_key].append(log.log_id)
        
    def get(self, log_id: str) -> Optional[LogEntry]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∞"""
        return self.logs.get(log_id)
        
    def delete(self, log_id: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –ª–æ–≥–∞"""
        log = self.logs.pop(log_id, None)
        if log:
            # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–æ–≤
            self.indexes["service"][log.service].remove(log_id)
            self.indexes["host"][log.host].remove(log_id)
            self.indexes["level"][log.level].remove(log_id)
            
    def count(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤"""
        return len(self.logs)


class LogSearchEngine:
    """–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫"""
    
    def __init__(self, storage: LogStorage):
        self.storage = storage
        
    def search(self, query: LogQuery) -> LogQueryResult:
        """–ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤"""
        start = datetime.now()
        
        result = LogQueryResult(query_id=query.query_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = self._get_candidates(query)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered = []
        for log_id in candidates:
            log = self.storage.get(log_id)
            if log and self._matches(log, query):
                filtered.append(log)
                
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
        filtered.sort(key=lambda x: x.timestamp, reverse=True)
        
        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
        result.total_count = len(filtered)
        result.logs = filtered[query.offset:query.offset + query.limit]
        result.returned_count = len(result.logs)
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏
        result.aggregations = self._aggregate(filtered)
        
        result.execution_time_ms = (datetime.now() - start).total_seconds() * 1000
        
        return result
        
    def _get_candidates(self, query: LogQuery) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        candidates = set(self.storage.logs.keys())
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ—Ä–≤–∏—Å–∞–º
        if query.services:
            service_logs = set()
            for service in query.services:
                service_logs.update(self.storage.indexes["service"].get(service, []))
            candidates &= service_logs
            
        # –§–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–≤–Ω—è–º
        if query.levels:
            level_logs = set()
            for level in query.levels:
                level_logs.update(self.storage.indexes["level"].get(level, []))
            candidates &= level_logs
            
        return candidates
        
    def _matches(self, log: LogEntry, query: LogQuery) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ª–æ–≥—É –∑–∞–ø—Ä–æ—Å—É"""
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        if query.text and query.text.lower() not in log.message.lower():
            return False
            
        # Regex –ø–æ–∏—Å–∫
        if query.regex:
            try:
                if not re.search(query.regex, log.message):
                    return False
            except:
                pass
                
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
        if query.start_time and log.timestamp < query.start_time:
            return False
        if query.end_time and log.timestamp > query.end_time:
            return False
            
        # –§–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–ª—è–º
        for field, value in query.field_filters.items():
            if log.fields.get(field) != value:
                return False
                
        return True
        
    def _aggregate(self, logs: List[LogEntry]) -> Dict[str, Any]:
        """–ê–≥—Ä–µ–≥–∞—Ü–∏–∏"""
        aggs = {
            "by_level": defaultdict(int),
            "by_service": defaultdict(int),
            "by_hour": defaultdict(int)
        }
        
        for log in logs:
            aggs["by_level"][log.level.name] += 1
            aggs["by_service"][log.service] += 1
            aggs["by_hour"][log.timestamp.strftime("%H:00")] += 1
            
        return {k: dict(v) for k, v in aggs.items()}


class PatternDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self):
        self.known_patterns: Dict[str, int] = defaultdict(int)
        self.anomalies: List[Dict[str, Any]] = []
        
    def analyze(self, logs: List[LogEntry]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        message_groups = defaultdict(list)
        
        for log in logs:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            normalized = self._normalize_message(log.message)
            message_groups[normalized].append(log)
            self.known_patterns[normalized] += 1
            
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        top_patterns = sorted(
            message_groups.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )[:10]
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        error_rate = sum(1 for l in logs if l.level in [LogLevel.ERROR, LogLevel.FATAL]) / len(logs) if logs else 0
        
        if error_rate > 0.1:  # > 10% –æ—à–∏–±–æ–∫
            self.anomalies.append({
                "type": "high_error_rate",
                "rate": error_rate,
                "timestamp": datetime.now()
            })
            
        return {
            "total_logs": len(logs),
            "unique_patterns": len(message_groups),
            "top_patterns": [(p, len(logs)) for p, logs in top_patterns],
            "error_rate": error_rate
        }
        
    def _normalize_message(self, message: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è"""
        # –ó–∞–º–µ–Ω—è–µ–º —á–∏—Å–ª–∞
        normalized = re.sub(r'\d+', 'N', message)
        # –ó–∞–º–µ–Ω—è–µ–º UUID
        normalized = re.sub(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}', 'UUID', normalized)
        # –ó–∞–º–µ–Ω—è–µ–º IP –∞–¥—Ä–µ—Å–∞
        normalized = re.sub(r'\d+\.\d+\.\d+\.\d+', 'IP', normalized)
        return normalized


class AlertEngine:
    """–î–≤–∏–∂–æ–∫ –∞–ª–µ—Ä—Ç–æ–≤"""
    
    def __init__(self):
        self.alerts: Dict[str, LogAlert] = {}
        self.triggered_alerts: List[Dict[str, Any]] = []
        
    def create_alert(self, name: str, query: str, threshold: int = 1,
                      time_window: int = 5, severity: str = "warning") -> LogAlert:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞"""
        alert = LogAlert(
            alert_id=f"alert_{uuid.uuid4().hex[:8]}",
            name=name,
            query=query,
            threshold=threshold,
            time_window_minutes=time_window,
            severity=severity
        )
        self.alerts[alert.alert_id] = alert
        return alert
        
    def check(self, logs: List[LogEntry]):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤"""
        for alert in self.alerts.values():
            # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            matches = 0
            for log in logs:
                if self._matches_query(log, alert.query):
                    matches += 1
                    
            alert.current_count = matches
            
            if matches >= alert.threshold:
                if alert.state != AlertState.FIRING:
                    alert.state = AlertState.FIRING
                    alert.last_triggered = datetime.now()
                    
                    self.triggered_alerts.append({
                        "alert_id": alert.alert_id,
                        "name": alert.name,
                        "count": matches,
                        "timestamp": datetime.now()
                    })
            else:
                if alert.state == AlertState.FIRING:
                    alert.state = AlertState.RESOLVED
                    
    def _matches_query(self, log: LogEntry, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–ø—Ä–æ—Å—É"""
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ
        return query.lower() in log.message.lower()


class RetentionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞"""
    
    def __init__(self, storage: LogStorage):
        self.storage = storage
        self.policies: Dict[str, RetentionPolicy] = {}
        self.archived: List[str] = []
        self.deleted_count: int = 0
        
    def add_policy(self, name: str, retention_days: int,
                    services: List[str] = None, **kwargs) -> RetentionPolicy:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏"""
        policy = RetentionPolicy(
            policy_id=f"policy_{uuid.uuid4().hex[:8]}",
            name=name,
            retention_days=retention_days,
            services=services or [],
            **kwargs
        )
        self.policies[policy.policy_id] = policy
        return policy
        
    async def apply_policies(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫"""
        now = datetime.now()
        
        for log_id, log in list(self.storage.logs.items()):
            policy = self._find_matching_policy(log)
            
            if policy:
                cutoff = now - timedelta(days=policy.retention_days)
                
                if log.timestamp < cutoff:
                    if policy.archive_before_delete:
                        self.archived.append(log_id)
                        
                    self.storage.delete(log_id)
                    self.deleted_count += 1
                    
    def _find_matching_policy(self, log: LogEntry) -> Optional[RetentionPolicy]:
        """–ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏"""
        for policy in self.policies.values():
            if policy.services and log.service not in policy.services:
                continue
            if policy.levels and log.level not in policy.levels:
                continue
            return policy
        return None


class LogAggregationPlatform:
    """–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.parser = LogParser()
        self.storage = LogStorage()
        self.search_engine = LogSearchEngine(self.storage)
        self.pattern_detector = PatternDetector()
        self.alert_engine = AlertEngine()
        self.retention_manager = RetentionManager(self.storage)
        
        self.streams: Dict[str, LogStream] = {}
        
    def create_stream(self, name: str, services: List[str] = None,
                       levels: List[LogLevel] = None) -> LogStream:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –ª–æ–≥–æ–≤"""
        stream = LogStream(
            stream_id=f"stream_{uuid.uuid4().hex[:8]}",
            name=name,
            services=services or [],
            levels=levels or []
        )
        self.streams[stream.stream_id] = stream
        return stream
        
    async def ingest(self, raw: str, service: str, host: str,
                      level: LogLevel = None, **kwargs) -> LogEntry:
        """–ü—Ä–∏—ë–º –ª–æ–≥–∞"""
        # –ü–∞—Ä—Å–∏–Ω–≥
        parsed, fields = self.parser.parse(raw)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∏–∑ –ø–æ–ª–µ–π –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π
        if not level:
            level_str = fields.get("level", fields.get("severity", "INFO")).upper()
            try:
                level = LogLevel[level_str]
            except:
                level = LogLevel.INFO
                
        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
        log = LogEntry(
            log_id=f"log_{uuid.uuid4().hex[:8]}",
            message=fields.get("message", raw),
            level=level,
            service=service,
            host=host,
            fields=fields,
            raw=raw,
            parsed=parsed,
            **kwargs
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self.storage.store(log)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫–∏
        for stream in self.streams.values():
            if self._matches_stream(log, stream):
                stream.buffer.append(log)
                stream.total_logs += 1
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –±—É—Ñ–µ—Ä
                if len(stream.buffer) > stream.buffer_size:
                    stream.buffer = stream.buffer[-stream.buffer_size:]
                    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–ª–µ—Ä—Ç—ã
        self.alert_engine.check([log])
        
        return log
        
    def _matches_stream(self, log: LogEntry, stream: LogStream) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫—É"""
        if stream.services and log.service not in stream.services:
            return False
        if stream.levels and log.level not in stream.levels:
            return False
        return True
        
    def search(self, text: str = "", services: List[str] = None,
                levels: List[LogLevel] = None, **kwargs) -> LogQueryResult:
        """–ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤"""
        query = LogQuery(
            query_id=f"query_{uuid.uuid4().hex[:8]}",
            text=text,
            services=services or [],
            levels=levels or [],
            **kwargs
        )
        return self.search_engine.search(query)
        
    def get_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        return {
            "total_logs": self.storage.count(),
            "streams": len(self.streams),
            "patterns": len(self.parser.patterns),
            "alerts": len(self.alert_engine.alerts),
            "triggered_alerts": len(self.alert_engine.triggered_alerts),
            "retention_policies": len(self.retention_manager.policies)
        }


# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–µ–º–æ-–ª–æ–≥–æ–≤
def generate_demo_log(service: str) -> Tuple[str, LogLevel]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–º–æ-–ª–æ–≥–∞"""
    templates = [
        ("INFO", f"Request processed successfully in {{}}ms", lambda: random.randint(10, 500)),
        ("DEBUG", f"Cache hit for key: user_{{}} ", lambda: random.randint(1000, 9999)),
        ("INFO", f"User {{}} logged in from IP {{}}", lambda: (random.randint(1, 1000), f"192.168.{random.randint(1, 254)}.{random.randint(1, 254)}")),
        ("WARN", f"Slow query detected: {{}}ms for query {{}}", lambda: (random.randint(1000, 5000), f"SELECT * FROM users WHERE id = {random.randint(1, 100)}")),
        ("ERROR", f"Failed to connect to database: timeout after {{}}s", lambda: random.randint(30, 120)),
        ("INFO", f"Health check passed, uptime: {{}} hours", lambda: random.randint(1, 720)),
        ("ERROR", f"OutOfMemoryError: Java heap space", lambda: None),
        ("FATAL", f"Service crashed: segmentation fault", lambda: None),
    ]
    
    level_str, template, gen = random.choice(templates)
    
    args = gen() if gen else None
    if args is not None:
        if isinstance(args, tuple):
            message = template.format(*args)
        else:
            message = template.format(args)
    else:
        message = template
        
    level = LogLevel[level_str]
    
    return message, level


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
if __name__ == "__main__":
    print("=" * 60)
    print("Server Init - Iteration 92: Log Aggregation Platform")
    print("=" * 60)
    
    async def demo():
        platform = LogAggregationPlatform()
        print("‚úì Log Aggregation Platform created")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤
        print("\nüìä Creating Log Streams...")
        
        all_logs = platform.create_stream("all_logs")
        errors_only = platform.create_stream(
            "errors_only",
            levels=[LogLevel.ERROR, LogLevel.FATAL]
        )
        api_logs = platform.create_stream(
            "api_logs",
            services=["api-gateway", "auth-service"]
        )
        
        print(f"  ‚úì Stream: {all_logs.name}")
        print(f"  ‚úì Stream: {errors_only.name} (errors & fatals only)")
        print(f"  ‚úì Stream: {api_logs.name} (api & auth services)")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
        print("\nüìù Log Parsers:")
        
        for name, pattern in platform.parser.patterns.items():
            print(f"  ‚úì {name}")
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤
        print("\nüö® Creating Alerts...")
        
        platform.alert_engine.create_alert(
            "High Error Rate",
            "error",
            threshold=5,
            time_window=5,
            severity="critical"
        )
        
        platform.alert_engine.create_alert(
            "Database Connection Issues",
            "failed to connect to database",
            threshold=3,
            time_window=1,
            severity="critical"
        )
        
        platform.alert_engine.create_alert(
            "Out of Memory",
            "OutOfMemoryError",
            threshold=1,
            time_window=1,
            severity="critical"
        )
        
        print(f"  ‚úì Created {len(platform.alert_engine.alerts)} alerts")
        
        # –ü–æ–ª–∏—Ç–∏–∫–∏ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞
        print("\nüì¶ Creating Retention Policies...")
        
        platform.retention_manager.add_policy(
            "default",
            retention_days=30
        )
        
        platform.retention_manager.add_policy(
            "errors_extended",
            retention_days=90,
            levels=[LogLevel.ERROR, LogLevel.FATAL]
        )
        
        print(f"  ‚úì Created {len(platform.retention_manager.policies)} retention policies")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–æ–≥–æ–≤
        print("\nüì• Ingesting Logs...")
        
        services = ["api-gateway", "auth-service", "user-service", "order-service", "payment-service"]
        hosts = ["server-01", "server-02", "server-03"]
        
        for _ in range(100):
            service = random.choice(services)
            host = random.choice(hosts)
            message, level = generate_demo_log(service)
            
            await platform.ingest(
                message,
                service=service,
                host=host,
                level=level,
                trace_id=uuid.uuid4().hex,
                request_id=f"req_{uuid.uuid4().hex[:8]}"
            )
            
        print(f"  ‚úì Ingested {platform.storage.count()} logs")
        
        # –°—Ç–∞—Ç—É—Å –ø–æ—Ç–æ–∫–æ–≤
        print("\nüìä Stream Status:")
        
        for stream in platform.streams.values():
            print(f"  {stream.name}: {stream.total_logs} logs")
            
        # –ü–æ–∏—Å–∫
        print("\nüîç Log Search Examples...")
        
        # –ü–æ–∏—Å–∫ –æ—à–∏–±–æ–∫
        print("\n  Searching for errors...")
        result = platform.search(levels=[LogLevel.ERROR, LogLevel.FATAL])
        
        print(f"    Found: {result.total_count} logs")
        print(f"    Execution time: {result.execution_time_ms:.2f}ms")
        
        if result.logs:
            print("\n    Sample Errors:")
            for log in result.logs[:3]:
                print(f"      [{log.level.name}] {log.service}: {log.message[:60]}...")
                
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É
        print("\n  Searching for 'database'...")
        result = platform.search(text="database")
        print(f"    Found: {result.total_count} logs")
        
        # –ü–æ–∏—Å–∫ –ø–æ —Å–µ—Ä–≤–∏—Å—É
        print("\n  Searching in api-gateway...")
        result = platform.search(services=["api-gateway"])
        print(f"    Found: {result.total_count} logs")
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏
        print("\n  Aggregations:")
        result = platform.search()
        
        if result.aggregations.get("by_level"):
            print("\n    By Level:")
            for level, count in sorted(result.aggregations["by_level"].items()):
                bar = "‚ñà" * (count // 5)
                print(f"      {level:>6}: {bar} ({count})")
                
        if result.aggregations.get("by_service"):
            print("\n    By Service:")
            for service, count in sorted(result.aggregations["by_service"].items(), key=lambda x: -x[1]):
                bar = "‚ñà" * (count // 5)
                print(f"      {service:>15}: {bar} ({count})")
                
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        print("\nüî¨ Pattern Analysis...")
        
        all_logs_list = list(platform.storage.logs.values())
        analysis = platform.pattern_detector.analyze(all_logs_list)
        
        print(f"\n  Total Logs: {analysis['total_logs']}")
        print(f"  Unique Patterns: {analysis['unique_patterns']}")
        print(f"  Error Rate: {analysis['error_rate']:.1%}")
        
        print("\n  Top Patterns:")
        for pattern, count in analysis["top_patterns"][:5]:
            short_pattern = pattern[:50] + "..." if len(pattern) > 50 else pattern
            print(f"    ({count:>3}) {short_pattern}")
            
        # –ê–ª–µ—Ä—Ç—ã
        print("\nüö® Alert Status:")
        
        for alert in platform.alert_engine.alerts.values():
            state_icon = {
                AlertState.PENDING: "‚è≥",
                AlertState.FIRING: "üî•",
                AlertState.RESOLVED: "‚úÖ"
            }.get(alert.state, "?")
            
            print(f"  {state_icon} {alert.name}")
            print(f"     Query: '{alert.query}'")
            print(f"     Threshold: {alert.threshold}")
            print(f"     Current: {alert.current_count}")
            print(f"     State: {alert.state.value}")
            
        if platform.alert_engine.triggered_alerts:
            print("\n  Triggered Alerts:")
            for trigger in platform.alert_engine.triggered_alerts[-5:]:
                print(f"    üîî {trigger['name']}: {trigger['count']} matches")
                
        # –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∏–º–µ—Ä–æ–≤
        print("\nüìã Log Parsing Examples...")
        
        examples = [
            '{"level": "INFO", "message": "User logged in", "user_id": 123}',
            '192.168.1.100 - - [15/Jan/2024:10:30:00 +0000] "GET /api/users HTTP/1.1" 200 1234',
            '<14>Jan 15 10:30:00 server-01 nginx: connection accepted'
        ]
        
        for raw in examples:
            parsed, fields = platform.parser.parse(raw)
            print(f"\n  Raw: {raw[:60]}...")
            print(f"  Parsed: {parsed}")
            if fields:
                print(f"  Fields: {dict(list(fields.items())[:3])}...")
                
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\nüìà Platform Statistics:")
        
        stats = platform.get_statistics()
        
        print(f"\n  Total Logs: {stats['total_logs']}")
        print(f"  Streams: {stats['streams']}")
        print(f"  Patterns: {stats['patterns']}")
        print(f"  Alerts: {stats['alerts']}")
        print(f"  Triggered: {stats['triggered_alerts']}")
        print(f"  Retention Policies: {stats['retention_policies']}")
        
        # Dashboard
        print("\nüìã Log Aggregation Dashboard:")
        print("  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
        print("  ‚îÇ              Log Aggregation Overview                       ‚îÇ")
        print("  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
        print(f"  ‚îÇ Total Logs:     {stats['total_logs']:>6}                               ‚îÇ")
        print(f"  ‚îÇ Active Streams: {stats['streams']:>6}                               ‚îÇ")
        print(f"  ‚îÇ Alert Rules:    {stats['alerts']:>6}                               ‚îÇ")
        print(f"  ‚îÇ Triggered:      {stats['triggered_alerts']:>6}                               ‚îÇ")
        print("  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
        
    asyncio.run(demo())
    
    print("\n" + "=" * 60)
    print("Log Aggregation Platform initialized!")
    print("=" * 60)
