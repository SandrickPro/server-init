#!/usr/bin/env python3
"""
Server Init - Iteration 48: Log Management & Analytics
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–∞–º–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- Log Aggregation - –∞–≥—Ä–µ–≥–∞—Ü–∏—è –ª–æ–≥–æ–≤
- Log Parsing & Enrichment - –ø–∞—Ä—Å–∏–Ω–≥ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
- Full-Text Search - –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
- Log Analytics - –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ª–æ–≥–æ–≤
- Anomaly Detection - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
- Alert Rules - –ø—Ä–∞–≤–∏–ª–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π
- Log Retention - –ø–æ–ª–∏—Ç–∏–∫–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è
- Dashboard & Visualization - –¥–∞—à–±–æ—Ä–¥—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
"""

import json
import asyncio
import hashlib
import time
import os
import re
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Set, Callable, Tuple, Pattern
from enum import Enum
from abc import ABC, abstractmethod
import random
from collections import defaultdict
import uuid


class LogLevel(Enum):
    """–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    TRACE = "trace"
    DEBUG = "debug"
    INFO = "info"
    WARN = "warn"
    ERROR = "error"
    FATAL = "fatal"


class LogSource(Enum):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –ª–æ–≥–æ–≤"""
    APPLICATION = "application"
    CONTAINER = "container"
    KUBERNETES = "kubernetes"
    SYSTEM = "system"
    NETWORK = "network"
    SECURITY = "security"
    DATABASE = "database"
    CUSTOM = "custom"


class AlertSeverity(Enum):
    """–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"


@dataclass
class LogEntry:
    """–ó–∞–ø–∏—Å—å –ª–æ–≥–∞"""
    log_id: str
    timestamp: datetime
    level: LogLevel
    message: str
    
    # –ò—Å—Ç–æ—á–Ω–∏–∫
    source: LogSource = LogSource.APPLICATION
    service: str = ""
    host: str = ""
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    labels: Dict[str, str] = field(default_factory=dict)
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    fields: Dict[str, Any] = field(default_factory=dict)
    
    # Trace context
    trace_id: Optional[str] = None
    span_id: Optional[str] = None
    
    # Raw
    raw: str = ""


@dataclass
class LogStream:
    """–ü–æ—Ç–æ–∫ –ª–æ–≥–æ–≤"""
    stream_id: str
    name: str
    
    # –õ–µ–π–±–ª—ã
    labels: Dict[str, str] = field(default_factory=dict)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    entries_count: int = 0
    bytes_count: int = 0
    
    # –í—Ä–µ–º—è
    first_entry_at: Optional[datetime] = None
    last_entry_at: Optional[datetime] = None


@dataclass
class ParseRule:
    """–ü—Ä–∞–≤–∏–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    rule_id: str
    name: str
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω
    pattern: str = ""
    pattern_type: str = "regex"  # regex, json, grok
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º—ã–µ –ø–æ–ª—è
    fields: List[str] = field(default_factory=list)
    
    # –£—Å–ª–æ–≤–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
    source_filter: Optional[str] = None
    level_filter: Optional[LogLevel] = None
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    matches: int = 0


@dataclass
class SearchQuery:
    """–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
    query_id: str
    query_string: str
    
    # –§–∏–ª—å—Ç—Ä—ã
    time_range: Tuple[datetime, datetime] = field(default_factory=lambda: (datetime.now() - timedelta(hours=1), datetime.now()))
    sources: List[LogSource] = field(default_factory=list)
    levels: List[LogLevel] = field(default_factory=list)
    services: List[str] = field(default_factory=list)
    
    # –õ–∏–º–∏—Ç—ã
    limit: int = 1000
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results: List[LogEntry] = field(default_factory=list)
    total_hits: int = 0
    execution_time_ms: float = 0.0


@dataclass
class AlertRule:
    """–ü—Ä–∞–≤–∏–ª–æ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
    rule_id: str
    name: str
    
    # –£—Å–ª–æ–≤–∏–µ
    query: str = ""
    condition: str = "count"  # count, rate, pattern
    threshold: float = 0.0
    operator: str = ">"  # >, <, >=, <=, ==
    
    # –ü–µ—Ä–∏–æ–¥
    evaluation_window: timedelta = field(default_factory=lambda: timedelta(minutes=5))
    
    # –û–ø–æ–≤–µ—â–µ–Ω–∏–µ
    severity: AlertSeverity = AlertSeverity.WARNING
    notification_channels: List[str] = field(default_factory=list)
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ
    enabled: bool = True
    last_triggered: Optional[datetime] = None
    trigger_count: int = 0


@dataclass
class Alert:
    """–û–ø–æ–≤–µ—â–µ–Ω–∏–µ"""
    alert_id: str
    rule_id: str
    
    # –î–∞–Ω–Ω—ã–µ
    title: str = ""
    message: str = ""
    severity: AlertSeverity = AlertSeverity.WARNING
    
    # –í—Ä–µ–º—è
    triggered_at: datetime = field(default_factory=datetime.now)
    resolved_at: Optional[datetime] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    value: float = 0.0
    threshold: float = 0.0
    
    # –°—Ç–∞—Ç—É—Å
    status: str = "firing"  # firing, resolved, acknowledged


@dataclass
class RetentionPolicy:
    """–ü–æ–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤"""
    policy_id: str
    name: str
    
    # –£—Å–ª–æ–≤–∏—è
    retention_days: int = 30
    source_filter: Optional[LogSource] = None
    level_filter: Optional[LogLevel] = None
    
    # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è
    archive_before_delete: bool = False
    archive_destination: str = ""
    
    # –°—Ç–∞—Ç—É—Å
    enabled: bool = True


@dataclass
class Dashboard:
    """–î–∞—à–±–æ—Ä–¥"""
    dashboard_id: str
    name: str
    
    # –ü–∞–Ω–µ–ª–∏
    panels: List[Dict[str, Any]] = field(default_factory=list)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    refresh_interval: int = 30  # seconds
    time_range: str = "1h"
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


class LogParser:
    """–ü–∞—Ä—Å–µ—Ä –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.rules: Dict[str, ParseRule] = {}
        self.grok_patterns: Dict[str, str] = {
            "TIMESTAMP": r"\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?",
            "LOGLEVEL": r"(?:TRACE|DEBUG|INFO|WARN|ERROR|FATAL)",
            "IP": r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}",
            "UUID": r"[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}",
            "NUMBER": r"\d+(?:\.\d+)?",
            "WORD": r"\w+",
            "GREEDYDATA": r".*"
        }
        
    def add_rule(self, rule: ParseRule):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        self.rules[rule.rule_id] = rule
        
    def parse(self, raw_log: str, source: LogSource = LogSource.APPLICATION) -> LogEntry:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–∞"""
        log_id = f"log_{uuid.uuid4().hex[:12]}"
        
        # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        entry = LogEntry(
            log_id=log_id,
            timestamp=datetime.now(),
            level=LogLevel.INFO,
            message=raw_log,
            source=source,
            raw=raw_log
        )
        
        # –ü–æ–ø—ã—Ç–∫–∞ JSON –ø–∞—Ä—Å–∏–Ω–≥–∞
        if raw_log.strip().startswith("{"):
            try:
                data = json.loads(raw_log)
                entry.message = data.get("message", data.get("msg", raw_log))
                entry.level = self._parse_level(data.get("level", data.get("severity", "info")))
                entry.fields = {k: v for k, v in data.items() if k not in ["message", "msg", "level", "severity"]}
                
                if "timestamp" in data:
                    entry.timestamp = self._parse_timestamp(data["timestamp"])
                if "service" in data:
                    entry.service = data["service"]
                if "trace_id" in data:
                    entry.trace_id = data["trace_id"]
                    
            except json.JSONDecodeError:
                pass
                
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –ø–∞—Ä—Å–∏–Ω–≥–∞
        for rule in self.rules.values():
            if rule.source_filter and rule.source_filter != source.value:
                continue
                
            match = self._apply_rule(rule, raw_log)
            if match:
                entry.fields.update(match)
                rule.matches += 1
                
        return entry
        
    def _parse_level(self, level_str: str) -> LogLevel:
        """–ü–∞—Ä—Å–∏–Ω–≥ —É—Ä–æ–≤–Ω—è"""
        level_map = {
            "trace": LogLevel.TRACE,
            "debug": LogLevel.DEBUG,
            "info": LogLevel.INFO,
            "warn": LogLevel.WARN,
            "warning": LogLevel.WARN,
            "error": LogLevel.ERROR,
            "fatal": LogLevel.FATAL,
            "critical": LogLevel.FATAL
        }
        return level_map.get(level_str.lower(), LogLevel.INFO)
        
    def _parse_timestamp(self, ts: Any) -> datetime:
        """–ü–∞—Ä—Å–∏–Ω–≥ timestamp"""
        if isinstance(ts, (int, float)):
            return datetime.fromtimestamp(ts)
        if isinstance(ts, str):
            try:
                return datetime.fromisoformat(ts.replace("Z", "+00:00"))
            except:
                pass
        return datetime.now()
        
    def _apply_rule(self, rule: ParseRule, raw_log: str) -> Optional[Dict[str, Any]]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if rule.pattern_type == "regex":
            try:
                match = re.search(rule.pattern, raw_log)
                if match:
                    return match.groupdict()
            except:
                pass
        return None


class LogStore:
    """–•—Ä–∞–Ω–∏–ª–∏—â–µ –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.entries: List[LogEntry] = []
        self.streams: Dict[str, LogStream] = {}
        self.index: Dict[str, List[int]] = defaultdict(list)  # –ò–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
        
    def append(self, entry: LogEntry):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        idx = len(self.entries)
        self.entries.append(entry)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self._update_index(entry, idx)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ stream
        stream_id = self._get_stream_id(entry)
        if stream_id not in self.streams:
            self.streams[stream_id] = LogStream(
                stream_id=stream_id,
                name=entry.service or "default",
                labels=entry.labels
            )
            
        stream = self.streams[stream_id]
        stream.entries_count += 1
        stream.bytes_count += len(entry.raw)
        stream.last_entry_at = entry.timestamp
        
        if not stream.first_entry_at:
            stream.first_entry_at = entry.timestamp
            
    def _update_index(self, entry: LogEntry, idx: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        # –ò–Ω–¥–µ–∫—Å –ø–æ —É—Ä–æ–≤–Ω—é
        self.index[f"level:{entry.level.value}"].append(idx)
        
        # –ò–Ω–¥–µ–∫—Å –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        self.index[f"source:{entry.source.value}"].append(idx)
        
        # –ò–Ω–¥–µ–∫—Å –ø–æ —Å–µ—Ä–≤–∏—Å—É
        if entry.service:
            self.index[f"service:{entry.service}"].append(idx)
            
        # –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π)
        words = entry.message.lower().split()
        for word in words[:50]:  # –õ–∏–º–∏—Ç —Å–ª–æ–≤
            if len(word) > 2:
                self.index[f"text:{word}"].append(idx)
                
    def _get_stream_id(self, entry: LogEntry) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ ID –ø–æ—Ç–æ–∫–∞"""
        labels = sorted(entry.labels.items())
        label_str = ",".join(f"{k}={v}" for k, v in labels)
        return hashlib.md5(f"{entry.source.value}:{entry.service}:{label_str}".encode()).hexdigest()[:16]
        
    def query(self, query: SearchQuery) -> SearchQuery:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        start_time = time.time()
        
        results = []
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å—É
        candidate_indices = None
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–≤–Ω—è–º
        if query.levels:
            level_indices = set()
            for level in query.levels:
                level_indices.update(self.index.get(f"level:{level.value}", []))
            candidate_indices = level_indices
            
        # –§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        if query.sources:
            source_indices = set()
            for source in query.sources:
                source_indices.update(self.index.get(f"source:{source.value}", []))
            if candidate_indices is not None:
                candidate_indices &= source_indices
            else:
                candidate_indices = source_indices
                
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        if query.query_string:
            text_indices = set()
            words = query.query_string.lower().split()
            for word in words:
                text_indices.update(self.index.get(f"text:{word}", []))
            if candidate_indices is not None:
                candidate_indices &= text_indices
            else:
                candidate_indices = text_indices
                
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤, –±–µ—Ä—ë–º –≤—Å–µ
        if candidate_indices is None:
            candidate_indices = set(range(len(self.entries)))
            
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        for idx in candidate_indices:
            if idx >= len(self.entries):
                continue
                
            entry = self.entries[idx]
            
            if entry.timestamp < query.time_range[0] or entry.timestamp > query.time_range[1]:
                continue
                
            results.append(entry)
            
            if len(results) >= query.limit:
                break
                
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        results.sort(key=lambda e: e.timestamp, reverse=True)
        
        query.results = results[:query.limit]
        query.total_hits = len(results)
        query.execution_time_ms = (time.time() - start_time) * 1000
        
        return query
        
    def get_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if not self.entries:
            return {"entries": 0}
            
        level_counts = defaultdict(int)
        source_counts = defaultdict(int)
        
        for entry in self.entries:
            level_counts[entry.level.value] += 1
            source_counts[entry.source.value] += 1
            
        return {
            "entries": len(self.entries),
            "streams": len(self.streams),
            "by_level": dict(level_counts),
            "by_source": dict(source_counts),
            "time_range": {
                "from": min(e.timestamp for e in self.entries).isoformat(),
                "to": max(e.timestamp for e in self.entries).isoformat()
            }
        }


class AnomalyDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –≤ –ª–æ–≥–∞—Ö"""
    
    def __init__(self):
        self.baselines: Dict[str, Dict[str, float]] = {}
        self.anomalies: List[Dict[str, Any]] = []
        
    def train_baseline(self, entries: List[LogEntry], window: timedelta = timedelta(hours=1)):
        """–û–±—É—á–µ–Ω–∏–µ baseline"""
        # –ü–æ–¥—Å—á—ë—Ç –ø–æ —É—Ä–æ–≤–Ω—è–º
        level_counts = defaultdict(int)
        error_rate = 0
        
        for entry in entries:
            level_counts[entry.level.value] += 1
            
        total = len(entries)
        
        if total > 0:
            error_rate = (level_counts.get("error", 0) + level_counts.get("fatal", 0)) / total
            
        self.baselines["default"] = {
            "avg_rate": total / (window.total_seconds() / 60),  # logs per minute
            "error_rate": error_rate,
            "level_distribution": {k: v / total for k, v in level_counts.items()}
        }
        
    def detect(self, entries: List[LogEntry], window: timedelta = timedelta(minutes=5)) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
        if "default" not in self.baselines:
            return []
            
        baseline = self.baselines["default"]
        detected = []
        
        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        level_counts = defaultdict(int)
        for entry in entries:
            level_counts[entry.level.value] += 1
            
        total = len(entries)
        
        if total == 0:
            return []
            
        current_error_rate = (level_counts.get("error", 0) + level_counts.get("fatal", 0)) / total
        current_rate = total / (window.total_seconds() / 60)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π
        # –í—Å–ø–ª–µ—Å–∫ –æ—à–∏–±–æ–∫
        if current_error_rate > baseline["error_rate"] * 2:
            anomaly = {
                "type": "error_spike",
                "severity": "high",
                "message": f"Error rate increased: {current_error_rate:.2%} vs baseline {baseline['error_rate']:.2%}",
                "detected_at": datetime.now().isoformat()
            }
            detected.append(anomaly)
            
        # –í—Å–ø–ª–µ—Å–∫ –æ–±—ä—ë–º–∞
        if current_rate > baseline["avg_rate"] * 3:
            anomaly = {
                "type": "volume_spike",
                "severity": "medium",
                "message": f"Log volume increased: {current_rate:.1f}/min vs baseline {baseline['avg_rate']:.1f}/min",
                "detected_at": datetime.now().isoformat()
            }
            detected.append(anomaly)
            
        # –ü–∞–¥–µ–Ω–∏–µ –æ–±—ä—ë–º–∞ (–≤–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞)
        if current_rate < baseline["avg_rate"] * 0.1 and baseline["avg_rate"] > 1:
            anomaly = {
                "type": "volume_drop",
                "severity": "medium",
                "message": f"Log volume dropped: {current_rate:.1f}/min vs baseline {baseline['avg_rate']:.1f}/min",
                "detected_at": datetime.now().isoformat()
            }
            detected.append(anomaly)
            
        self.anomalies.extend(detected)
        return detected


class AlertManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –æ–ø–æ–≤–µ—â–µ–Ω–∏–π"""
    
    def __init__(self):
        self.rules: Dict[str, AlertRule] = {}
        self.alerts: List[Alert] = []
        self.notification_handlers: Dict[str, Callable] = {}
        
    def add_rule(self, rule: AlertRule):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        self.rules[rule.rule_id] = rule
        
    def register_notification_handler(self, channel: str, handler: Callable):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π"""
        self.notification_handlers[channel] = handler
        
    async def evaluate(self, store: LogStore) -> List[Alert]:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–∞–≤–∏–ª"""
        triggered = []
        
        now = datetime.now()
        
        for rule in self.rules.values():
            if not rule.enabled:
                continue
                
            # –ó–∞–ø—Ä–æ—Å –ª–æ–≥–æ–≤ –¥–ª—è –ø—Ä–∞–≤–∏–ª–∞
            query = SearchQuery(
                query_id=f"alert_query_{rule.rule_id}",
                query_string=rule.query,
                time_range=(now - rule.evaluation_window, now)
            )
            
            store.query(query)
            
            # –û—Ü–µ–Ω–∫–∞ —É—Å–ª–æ–≤–∏—è
            value = len(query.results)
            
            if rule.condition == "rate":
                value = value / (rule.evaluation_window.total_seconds() / 60)
                
            should_trigger = self._evaluate_condition(value, rule.threshold, rule.operator)
            
            if should_trigger:
                alert = Alert(
                    alert_id=f"alert_{uuid.uuid4().hex[:8]}",
                    rule_id=rule.rule_id,
                    title=f"Alert: {rule.name}",
                    message=f"Rule triggered: {rule.query} {rule.operator} {rule.threshold}",
                    severity=rule.severity,
                    value=value,
                    threshold=rule.threshold
                )
                
                self.alerts.append(alert)
                triggered.append(alert)
                
                rule.last_triggered = now
                rule.trigger_count += 1
                
                # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π
                await self._send_notifications(alert, rule.notification_channels)
                
        return triggered
        
    def _evaluate_condition(self, value: float, threshold: float, operator: str) -> bool:
        """–û—Ü–µ–Ω–∫–∞ —É—Å–ª–æ–≤–∏—è"""
        ops = {
            ">": lambda v, t: v > t,
            "<": lambda v, t: v < t,
            ">=": lambda v, t: v >= t,
            "<=": lambda v, t: v <= t,
            "==": lambda v, t: v == t
        }
        return ops.get(operator, lambda v, t: False)(value, threshold)
        
    async def _send_notifications(self, alert: Alert, channels: List[str]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π"""
        for channel in channels:
            handler = self.notification_handlers.get(channel)
            if handler:
                try:
                    await handler(alert)
                except:
                    pass
                    
    def get_active_alerts(self) -> List[Alert]:
        """–ê–∫—Ç–∏–≤–Ω—ã–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        return [a for a in self.alerts if a.status == "firing"]
        
    def acknowledge_alert(self, alert_id: str):
        """–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        for alert in self.alerts:
            if alert.alert_id == alert_id:
                alert.status = "acknowledged"
                break
                
    def resolve_alert(self, alert_id: str):
        """–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        for alert in self.alerts:
            if alert.alert_id == alert_id:
                alert.status = "resolved"
                alert.resolved_at = datetime.now()
                break


class LogManagementPlatform:
    """–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª–æ–≥–∞–º–∏"""
    
    def __init__(self):
        self.parser = LogParser()
        self.store = LogStore()
        self.anomaly_detector = AnomalyDetector()
        self.alert_manager = AlertManager()
        
        # Retention policies
        self.retention_policies: Dict[str, RetentionPolicy] = {}
        
        # Dashboards
        self.dashboards: Dict[str, Dashboard] = {}
        
    def ingest(self, raw_logs: List[str], source: LogSource = LogSource.APPLICATION):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ–≤"""
        for raw in raw_logs:
            entry = self.parser.parse(raw, source)
            self.store.append(entry)
            
    def search(self, query_string: str, 
               time_range: Optional[Tuple[datetime, datetime]] = None,
               **filters) -> SearchQuery:
        """–ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤"""
        query = SearchQuery(
            query_id=f"query_{uuid.uuid4().hex[:8]}",
            query_string=query_string,
            time_range=time_range or (datetime.now() - timedelta(hours=1), datetime.now()),
            sources=[LogSource(s) for s in filters.get("sources", [])],
            levels=[LogLevel(l) for l in filters.get("levels", [])],
            services=filters.get("services", []),
            limit=filters.get("limit", 1000)
        )
        
        return self.store.query(query)
        
    def add_parse_rule(self, name: str, pattern: str, 
                        fields: List[str], **kwargs) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        rule = ParseRule(
            rule_id=f"parse_{uuid.uuid4().hex[:8]}",
            name=name,
            pattern=pattern,
            fields=fields,
            **kwargs
        )
        self.parser.add_rule(rule)
        return rule.rule_id
        
    def add_alert_rule(self, name: str, query: str,
                        threshold: float, **kwargs) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        rule = AlertRule(
            rule_id=f"alert_{uuid.uuid4().hex[:8]}",
            name=name,
            query=query,
            threshold=threshold,
            **kwargs
        )
        self.alert_manager.add_rule(rule)
        return rule.rule_id
        
    def create_dashboard(self, name: str, panels: List[Dict[str, Any]]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞"""
        dashboard = Dashboard(
            dashboard_id=f"dash_{uuid.uuid4().hex[:8]}",
            name=name,
            panels=panels
        )
        self.dashboards[dashboard.dashboard_id] = dashboard
        return dashboard.dashboard_id
        
    async def detect_anomalies(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
        recent_entries = self.store.entries[-1000:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 1000 –∑–∞–ø–∏—Å–µ–π
        
        # –û–±—É—á–µ–Ω–∏–µ baseline –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        if not self.anomaly_detector.baselines:
            self.anomaly_detector.train_baseline(recent_entries)
            
        return self.anomaly_detector.detect(recent_entries)
        
    async def evaluate_alerts(self) -> List[Alert]:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–∞–≤–∏–ª –æ–ø–æ–≤–µ—â–µ–Ω–∏–π"""
        return await self.alert_manager.evaluate(self.store)
        
    def get_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
        return {
            "store": self.store.get_statistics(),
            "parse_rules": len(self.parser.rules),
            "alert_rules": len(self.alert_manager.rules),
            "active_alerts": len(self.alert_manager.get_active_alerts()),
            "dashboards": len(self.dashboards),
            "anomalies_detected": len(self.anomaly_detector.anomalies)
        }


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
if __name__ == "__main__":
    print("=" * 60)
    print("Server Init - Iteration 48: Log Management & Analytics")
    print("=" * 60)
    
    async def demo():
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        platform = LogManagementPlatform()
        print("‚úì Log Management Platform created")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤
        print("\nüìù Ingesting logs...")
        
        sample_logs = []
        
        services = ["api-gateway", "user-service", "order-service", "payment-service"]
        levels = ["info", "info", "info", "info", "warn", "error"]  # Weighted
        
        for i in range(500):
            service = random.choice(services)
            level = random.choice(levels)
            
            if level == "error":
                messages = [
                    "Database connection timeout",
                    "Failed to process request",
                    "Authentication failed",
                    "Invalid input data"
                ]
            elif level == "warn":
                messages = [
                    "High memory usage detected",
                    "Slow query execution",
                    "Rate limit approaching"
                ]
            else:
                messages = [
                    "Request processed successfully",
                    "User logged in",
                    "Order created",
                    "Health check passed"
                ]
                
            log = json.dumps({
                "timestamp": (datetime.now() - timedelta(minutes=random.randint(0, 60))).isoformat(),
                "level": level,
                "service": service,
                "message": random.choice(messages),
                "trace_id": uuid.uuid4().hex[:16],
                "duration_ms": random.randint(10, 500)
            })
            
            sample_logs.append(log)
            
        platform.ingest(sample_logs, LogSource.APPLICATION)
        print(f"  ‚úì Ingested {len(sample_logs)} log entries")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = platform.store.get_statistics()
        print(f"\nüìä Log Statistics:")
        print(f"  Total entries: {stats['entries']}")
        print(f"  Streams: {stats['streams']}")
        print(f"  By level: {stats['by_level']}")
        
        # –ü–æ–∏—Å–∫
        print("\nüîç Search Examples...")
        
        # –ü–æ–∏—Å–∫ –æ—à–∏–±–æ–∫
        error_query = platform.search(
            query_string="error failed",
            levels=["error"]
        )
        print(f"  Error search: {error_query.total_hits} hits in {error_query.execution_time_ms:.2f}ms")
        
        # –ü–æ–∏—Å–∫ –ø–æ —Å–µ—Ä–≤–∏—Å—É
        service_query = platform.search(
            query_string="",
            services=["api-gateway"]
        )
        print(f"  Service search: {service_query.total_hits} hits")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
        print("\nüìã Parse Rules...")
        
        rule_id = platform.add_parse_rule(
            name="duration-extractor",
            pattern=r'"duration_ms":\s*(\d+)',
            fields=["duration"]
        )
        print(f"  ‚úì Added parse rule: {rule_id}")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –æ–ø–æ–≤–µ—â–µ–Ω–∏–π
        print("\nüîî Alert Rules...")
        
        alert_rule_id = platform.add_alert_rule(
            name="High Error Rate",
            query="error failed",
            threshold=10,
            operator=">",
            severity=AlertSeverity.WARNING,
            notification_channels=["slack", "email"]
        )
        print(f"  ‚úì Added alert rule: High Error Rate")
        
        critical_rule_id = platform.add_alert_rule(
            name="Critical Errors",
            query="fatal critical",
            threshold=1,
            operator=">=",
            severity=AlertSeverity.CRITICAL,
            notification_channels=["pagerduty"]
        )
        print(f"  ‚úì Added alert rule: Critical Errors")
        
        # –û—Ü–µ–Ω–∫–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π
        alerts = await platform.evaluate_alerts()
        print(f"\n  Triggered alerts: {len(alerts)}")
        
        for alert in alerts:
            print(f"    [{alert.severity.value}] {alert.title}")
            
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        print("\nüîÆ Anomaly Detection...")
        
        anomalies = await platform.detect_anomalies()
        print(f"  Anomalies detected: {len(anomalies)}")
        
        for anomaly in anomalies:
            print(f"    [{anomaly['severity']}] {anomaly['type']}: {anomaly['message']}")
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞
        print("\nüìä Dashboard...")
        
        dashboard_id = platform.create_dashboard(
            name="Application Logs Overview",
            panels=[
                {
                    "title": "Log Volume",
                    "type": "time_series",
                    "query": "*"
                },
                {
                    "title": "Error Rate",
                    "type": "gauge",
                    "query": "level:error"
                },
                {
                    "title": "Top Services",
                    "type": "bar_chart",
                    "query": "*",
                    "group_by": "service"
                },
                {
                    "title": "Recent Errors",
                    "type": "table",
                    "query": "level:error",
                    "limit": 10
                }
            ]
        )
        print(f"  ‚úì Created dashboard: {dashboard_id}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = platform.get_statistics()
        print(f"\nüìà Platform Statistics:")
        print(f"  Log entries: {final_stats['store']['entries']}")
        print(f"  Parse rules: {final_stats['parse_rules']}")
        print(f"  Alert rules: {final_stats['alert_rules']}")
        print(f"  Active alerts: {final_stats['active_alerts']}")
        print(f"  Dashboards: {final_stats['dashboards']}")
        
    asyncio.run(demo())
    
    print("\n" + "=" * 60)
    print("Log Management & Analytics Platform initialized!")
    print("=" * 60)
