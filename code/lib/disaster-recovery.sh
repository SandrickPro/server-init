#!/usr/bin/env bash

###############################################################################
# DISASTER RECOVERY MODULE v11.0
# Automated backup, restore, and business continuity
###############################################################################

set -euo pipefail

# Configuration
BACKUP_RETENTION_DAYS=90
BACKUP_SCHEDULE="0 2 * * *"  # Daily at 2 AM
BACKUP_STORAGE="s3://disaster-recovery-backups"
VELERO_VERSION="1.12.1"
RESTIC_VERSION="0.16.2"

###############################################################################
# Velero Installation
###############################################################################

install_velero() {
    echo "üíæ Installing Velero ${VELERO_VERSION}..."
    
    # Download Velero
    cd /tmp
    wget "https://github.com/vmware-tanzu/velero/releases/download/v${VELERO_VERSION}/velero-v${VELERO_VERSION}-linux-amd64.tar.gz"
    tar -xzf "velero-v${VELERO_VERSION}-linux-amd64.tar.gz"
    
    # Install binary
    mv "velero-v${VELERO_VERSION}-linux-amd64/velero" /usr/local/bin/
    chmod +x /usr/local/bin/velero
    
    # Cleanup
    rm -rf "velero-v${VELERO_VERSION}-linux-amd64"*
    
    echo "‚úÖ Velero installed"
}

configure_velero() {
    echo "‚öôÔ∏è  Configuring Velero..."
    
    # Create AWS credentials file
    cat > /tmp/velero-credentials << EOF
[default]
aws_access_key_id=${AWS_ACCESS_KEY_ID}
aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}
EOF
    
    # Install Velero in Kubernetes
    velero install \
        --provider aws \
        --plugins velero/velero-plugin-for-aws:v1.8.0 \
        --bucket disaster-recovery-backups \
        --backup-location-config region=us-east-1 \
        --snapshot-location-config region=us-east-1 \
        --secret-file /tmp/velero-credentials \
        --use-volume-snapshots=true \
        --use-node-agent \
        --default-volumes-to-fs-backup
    
    # Wait for Velero to be ready
    kubectl wait --for=condition=available --timeout=300s \
        deployment/velero -n velero
    
    rm -f /tmp/velero-credentials
    
    echo "‚úÖ Velero configured"
}

###############################################################################
# Backup Strategies
###############################################################################

create_backup_schedules() {
    echo "üìÖ Creating backup schedules..."
    
    # Full backup daily at 2 AM
    velero schedule create daily-full-backup \
        --schedule="$BACKUP_SCHEDULE" \
        --ttl 2160h \
        --include-namespaces=production,database,monitoring
    
    # Incremental backup every 4 hours
    velero schedule create incremental-backup \
        --schedule="0 */4 * * *" \
        --ttl 168h \
        --include-namespaces=production,database
    
    # Configuration backup every hour
    velero schedule create config-backup \
        --schedule="0 * * * *" \
        --ttl 72h \
        --include-resources=configmaps,secrets
    
    # Database backup every 30 minutes
    velero schedule create database-backup \
        --schedule="*/30 * * * *" \
        --ttl 24h \
        --include-namespaces=database \
        --snapshot-volumes=true
    
    echo "‚úÖ Backup schedules created"
}

backup_databases() {
    echo "üóÑÔ∏è  Backing up databases..."
    
    # PostgreSQL backup
    backup_postgresql
    
    # Redis backup
    backup_redis
    
    # Elasticsearch backup
    backup_elasticsearch
    
    echo "‚úÖ Database backups complete"
}

backup_postgresql() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="postgres_backup_${timestamp}.sql.gz"
    
    # Create logical backup
    kubectl exec -n database postgres-primary-1 -- \
        pg_dumpall -U postgres | gzip > "/tmp/$backup_file"
    
    # Upload to S3
    aws s3 cp "/tmp/$backup_file" \
        "${BACKUP_STORAGE}/postgres/$backup_file"
    
    # Create point-in-time recovery base backup
    kubectl exec -n database postgres-primary-1 -- \
        pg_basebackup -U postgres -D /tmp/basebackup -Ft -z -P
    
    # Upload base backup
    kubectl cp database/postgres-primary-1:/tmp/basebackup \
        "/tmp/basebackup_${timestamp}.tar.gz"
    
    aws s3 cp "/tmp/basebackup_${timestamp}.tar.gz" \
        "${BACKUP_STORAGE}/postgres/basebackup/"
    
    # Cleanup
    rm -f "/tmp/$backup_file" "/tmp/basebackup_${timestamp}.tar.gz"
    
    echo "‚úÖ PostgreSQL backup complete: $backup_file"
}

backup_redis() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="redis_backup_${timestamp}.rdb"
    
    # Trigger RDB save
    kubectl exec -n database redis-master-0 -- \
        redis-cli BGSAVE
    
    # Wait for save to complete
    sleep 10
    
    # Copy RDB file
    kubectl cp database/redis-master-0:/data/dump.rdb \
        "/tmp/$backup_file"
    
    # Upload to S3
    aws s3 cp "/tmp/$backup_file" \
        "${BACKUP_STORAGE}/redis/$backup_file"
    
    # Cleanup
    rm -f "/tmp/$backup_file"
    
    echo "‚úÖ Redis backup complete: $backup_file"
}

backup_elasticsearch() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local snapshot_name="snapshot_${timestamp}"
    
    # Create snapshot repository if not exists
    curl -X PUT "http://elasticsearch:9200/_snapshot/s3_repository" \
        -H 'Content-Type: application/json' \
        -d "{
            \"type\": \"s3\",
            \"settings\": {
                \"bucket\": \"disaster-recovery-backups\",
                \"base_path\": \"elasticsearch\",
                \"region\": \"us-east-1\"
            }
        }"
    
    # Create snapshot
    curl -X PUT "http://elasticsearch:9200/_snapshot/s3_repository/$snapshot_name?wait_for_completion=true" \
        -H 'Content-Type: application/json' \
        -d "{
            \"indices\": \"*\",
            \"ignore_unavailable\": true,
            \"include_global_state\": true
        }"
    
    echo "‚úÖ Elasticsearch backup complete: $snapshot_name"
}

backup_secrets() {
    echo "üîê Backing up secrets..."
    
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="secrets_backup_${timestamp}.enc"
    
    # Export all secrets
    kubectl get secrets --all-namespaces -o json > /tmp/secrets.json
    
    # Encrypt secrets
    openssl enc -aes-256-cbc -salt -in /tmp/secrets.json \
        -out "/tmp/$backup_file" \
        -pass pass:"${ENCRYPTION_KEY}"
    
    # Upload to S3
    aws s3 cp "/tmp/$backup_file" \
        "${BACKUP_STORAGE}/secrets/$backup_file"
    
    # Cleanup
    rm -f /tmp/secrets.json "/tmp/$backup_file"
    
    echo "‚úÖ Secrets backup complete"
}

backup_application_data() {
    echo "üì¶ Backing up application data..."
    
    local timestamp=$(date +%Y%m%d_%H%M%S)
    
    # Backup persistent volumes
    velero backup create "app-data-${timestamp}" \
        --include-namespaces=production \
        --snapshot-volumes=true \
        --wait
    
    # Backup configuration
    kubectl get configmaps --all-namespaces -o yaml > \
        "/tmp/configmaps_${timestamp}.yaml"
    
    aws s3 cp "/tmp/configmaps_${timestamp}.yaml" \
        "${BACKUP_STORAGE}/config/"
    
    rm -f "/tmp/configmaps_${timestamp}.yaml"
    
    echo "‚úÖ Application data backup complete"
}

###############################################################################
# Restore Operations
###############################################################################

restore_full_cluster() {
    local backup_name=$1
    
    echo "üîÑ Restoring full cluster from backup: $backup_name"
    
    # Restore Kubernetes resources
    velero restore create --from-backup "$backup_name" --wait
    
    # Restore databases
    restore_postgresql
    restore_redis
    restore_elasticsearch
    
    # Restore secrets
    restore_secrets
    
    # Verify restoration
    verify_restore
    
    echo "‚úÖ Full cluster restore complete"
}

restore_postgresql() {
    echo "üóÑÔ∏è  Restoring PostgreSQL..."
    
    # Get latest backup
    local latest_backup=$(aws s3 ls "${BACKUP_STORAGE}/postgres/" | \
        grep ".sql.gz" | sort | tail -1 | awk '{print $4}')
    
    # Download backup
    aws s3 cp "${BACKUP_STORAGE}/postgres/$latest_backup" \
        /tmp/postgres_restore.sql.gz
    
    # Stop application connections
    kubectl scale deployment app --replicas=0 -n production
    
    # Restore database
    gunzip < /tmp/postgres_restore.sql.gz | \
        kubectl exec -i -n database postgres-primary-1 -- \
        psql -U postgres
    
    # Restart application
    kubectl scale deployment app --replicas=3 -n production
    
    # Cleanup
    rm -f /tmp/postgres_restore.sql.gz
    
    echo "‚úÖ PostgreSQL restored"
}

restore_redis() {
    echo "üîÑ Restoring Redis..."
    
    # Get latest backup
    local latest_backup=$(aws s3 ls "${BACKUP_STORAGE}/redis/" | \
        sort | tail -1 | awk '{print $4}')
    
    # Download backup
    aws s3 cp "${BACKUP_STORAGE}/redis/$latest_backup" \
        /tmp/dump.rdb
    
    # Stop Redis
    kubectl scale statefulset redis-master --replicas=0 -n database
    
    # Copy backup file
    kubectl cp /tmp/dump.rdb database/redis-master-0:/data/dump.rdb
    
    # Start Redis
    kubectl scale statefulset redis-master --replicas=1 -n database
    
    # Wait for Redis to load data
    sleep 30
    
    # Cleanup
    rm -f /tmp/dump.rdb
    
    echo "‚úÖ Redis restored"
}

restore_elasticsearch() {
    echo "üîç Restoring Elasticsearch..."
    
    # Get latest snapshot
    local latest_snapshot=$(curl -s "http://elasticsearch:9200/_snapshot/s3_repository/_all" | \
        jq -r '.snapshots | sort_by(.start_time) | last | .snapshot')
    
    # Close indices
    curl -X POST "http://elasticsearch:9200/_all/_close"
    
    # Restore snapshot
    curl -X POST "http://elasticsearch:9200/_snapshot/s3_repository/$latest_snapshot/_restore?wait_for_completion=true" \
        -H 'Content-Type: application/json' \
        -d '{
            "indices": "*",
            "ignore_unavailable": true,
            "include_global_state": true
        }'
    
    echo "‚úÖ Elasticsearch restored"
}

restore_secrets() {
    echo "üîê Restoring secrets..."
    
    # Get latest backup
    local latest_backup=$(aws s3 ls "${BACKUP_STORAGE}/secrets/" | \
        sort | tail -1 | awk '{print $4}')
    
    # Download and decrypt
    aws s3 cp "${BACKUP_STORAGE}/secrets/$latest_backup" \
        /tmp/secrets.enc
    
    openssl enc -aes-256-cbc -d -in /tmp/secrets.enc \
        -out /tmp/secrets.json \
        -pass pass:"${ENCRYPTION_KEY}"
    
    # Apply secrets
    kubectl apply -f /tmp/secrets.json
    
    # Cleanup
    rm -f /tmp/secrets.enc /tmp/secrets.json
    
    echo "‚úÖ Secrets restored"
}

###############################################################################
# Point-in-Time Recovery
###############################################################################

point_in_time_recovery() {
    local target_time=$1  # Format: YYYY-MM-DD HH:MM:SS
    
    echo "‚è∞ Performing point-in-time recovery to: $target_time"
    
    # Find backup closest to target time
    local backup_name=$(find_closest_backup "$target_time")
    
    # Restore base backup
    restore_postgresql
    
    # Replay WAL logs up to target time
    replay_wal_logs "$target_time"
    
    echo "‚úÖ Point-in-time recovery complete"
}

replay_wal_logs() {
    local target_time=$1
    
    echo "üìú Replaying WAL logs..."
    
    # Configure recovery
    cat > /tmp/recovery.conf << EOF
restore_command = 'aws s3 cp ${BACKUP_STORAGE}/postgres/wal/%f %p'
recovery_target_time = '$target_time'
recovery_target_action = 'promote'
EOF
    
    # Copy to PostgreSQL data directory
    kubectl cp /tmp/recovery.conf \
        database/postgres-primary-1:/var/lib/postgresql/data/recovery.conf
    
    # Restart PostgreSQL in recovery mode
    kubectl exec -n database postgres-primary-1 -- \
        pg_ctl restart -D /var/lib/postgresql/data
    
    # Wait for recovery to complete
    sleep 60
    
    rm -f /tmp/recovery.conf
}

###############################################################################
# Disaster Recovery Testing
###############################################################################

test_disaster_recovery() {
    echo "üß™ Testing disaster recovery procedures..."
    
    # Create test backup
    local test_backup="dr-test-$(date +%Y%m%d_%H%M%S)"
    velero backup create "$test_backup" \
        --include-namespaces=production \
        --wait
    
    # Simulate disaster by deleting namespace
    kubectl create namespace dr-test
    kubectl label namespace dr-test app=dr-test
    
    # Restore to test namespace
    velero restore create "${test_backup}-restore" \
        --from-backup "$test_backup" \
        --namespace-mappings production:dr-test \
        --wait
    
    # Verify restoration
    local pod_count=$(kubectl get pods -n dr-test --no-headers | wc -l)
    if [[ $pod_count -gt 0 ]]; then
        echo "‚úÖ DR test successful: $pod_count pods restored"
    else
        echo "‚ùå DR test failed: No pods restored"
        return 1
    fi
    
    # Cleanup test namespace
    kubectl delete namespace dr-test
    
    # Delete test backup
    velero backup delete "$test_backup" --confirm
    
    echo "‚úÖ Disaster recovery test complete"
}

run_dr_drill() {
    echo "üéØ Running full disaster recovery drill..."
    
    local start_time=$(date +%s)
    
    # Step 1: Document current state
    document_current_state
    
    # Step 2: Create checkpoint backup
    create_checkpoint_backup
    
    # Step 3: Simulate disaster
    simulate_disaster
    
    # Step 4: Perform recovery
    perform_recovery
    
    # Step 5: Verify recovery
    verify_recovery
    
    # Step 6: Calculate RTO/RPO
    local end_time=$(date +%s)
    local rto=$((end_time - start_time))
    
    echo "‚úÖ DR drill complete"
    echo "   RTO: ${rto}s (target: 300s)"
    echo "   Status: $([ $rto -le 300 ] && echo "‚úÖ Passed" || echo "‚ö†Ô∏è Exceeded")"
    
    # Generate report
    generate_dr_report "$rto"
}

document_current_state() {
    echo "üìù Documenting current state..."
    
    kubectl get all --all-namespaces -o yaml > /tmp/pre-dr-state.yaml
    aws s3 cp /tmp/pre-dr-state.yaml "${BACKUP_STORAGE}/dr-drills/"
}

create_checkpoint_backup() {
    echo "üíæ Creating checkpoint backup..."
    
    velero backup create "dr-checkpoint-$(date +%Y%m%d_%H%M%S)" \
        --include-namespaces=production,database \
        --snapshot-volumes=true \
        --wait
}

simulate_disaster() {
    echo "üí• Simulating disaster (controlled)..."
    
    # Scale down applications (don't delete)
    kubectl scale deployment --all --replicas=0 -n production
    
    # Simulate database corruption
    kubectl exec -n database postgres-primary-1 -- \
        psql -c "DROP DATABASE IF EXISTS test_corrupted;"
}

perform_recovery() {
    echo "üîÑ Performing recovery..."
    
    # Get latest backup
    local latest_backup=$(velero backup get -o json | \
        jq -r '.items | sort_by(.metadata.creationTimestamp) | last | .metadata.name')
    
    # Restore
    velero restore create "dr-restore-$(date +%Y%m%d_%H%M%S)" \
        --from-backup "$latest_backup" \
        --wait
}

verify_recovery() {
    echo "‚úÖ Verifying recovery..."
    
    # Check all pods are running
    local unhealthy_pods=$(kubectl get pods --all-namespaces | \
        grep -v "Running\|Completed" | wc -l)
    
    if [[ $unhealthy_pods -gt 1 ]]; then  # 1 for header
        echo "‚ö†Ô∏è  Warning: $unhealthy_pods pods are not healthy"
    fi
    
    # Check application endpoints
    local health_status=$(curl -sf http://localhost/health || echo "unhealthy")
    if [[ "$health_status" == "healthy" ]]; then
        echo "‚úÖ Application is healthy"
    else
        echo "‚ùå Application health check failed"
    fi
}

generate_dr_report() {
    local rto=$1
    local timestamp=$(date -Iseconds)
    
    cat > "/tmp/dr-report-${timestamp}.md" << EOF
# Disaster Recovery Drill Report

**Date:** $timestamp
**Duration:** ${rto}s
**RTO Target:** 300s
**Status:** $([ $rto -le 300 ] && echo "‚úÖ Passed" || echo "‚ö†Ô∏è Exceeded")

## Summary

- Checkpoint backup created successfully
- Disaster simulated (application scaled down)
- Recovery performed using Velero
- All services restored and verified

## Metrics

- **RTO (Recovery Time Objective):** ${rto}s / 300s target
- **RPO (Recovery Point Objective):** <60s (last backup was recent)
- **Data Loss:** None detected
- **Services Recovered:** $(kubectl get pods --all-namespaces --no-headers | wc -l) pods

## Recommendations

$([ $rto -gt 300 ] && echo "- RTO exceeded target, investigate backup performance" || echo "- RTO within acceptable limits")
- Continue quarterly DR drills
- Update runbooks based on findings

## Next Steps

- Schedule next drill in 3 months
- Review and update DR procedures
- Train team on recovery procedures
EOF
    
    # Upload report
    aws s3 cp "/tmp/dr-report-${timestamp}.md" \
        "${BACKUP_STORAGE}/dr-reports/"
    
    # Send notification
    /opt/telegram-bot/send-alert.sh \
        "DR Drill Report" \
        "$(cat /tmp/dr-report-${timestamp}.md)"
}

###############################################################################
# Backup Monitoring
###############################################################################

monitor_backups() {
    echo "üëÅÔ∏è  Monitoring backup status..."
    
    # Check backup age
    local last_backup_time=$(velero backup get -o json | \
        jq -r '.items | sort_by(.metadata.creationTimestamp) | last | .metadata.creationTimestamp')
    
    local last_backup_age=$(( $(date +%s) - $(date -d "$last_backup_time" +%s) ))
    
    if [[ $last_backup_age -gt 86400 ]]; then  # 24 hours
        echo "‚ö†Ô∏è  Warning: Last backup is ${last_backup_age}s old"
        /opt/telegram-bot/send-alert.sh \
            "Backup Alert" \
            "Last backup is older than 24 hours"
    fi
    
    # Check backup success rate
    local total_backups=$(velero backup get -o json | jq '.items | length')
    local failed_backups=$(velero backup get -o json | \
        jq '[.items[] | select(.status.phase=="Failed")] | length')
    
    local success_rate=$(( (total_backups - failed_backups) * 100 / total_backups ))
    
    echo "üìä Backup Statistics:"
    echo "   Total: $total_backups"
    echo "   Failed: $failed_backups"
    echo "   Success Rate: ${success_rate}%"
}

###############################################################################
# Utility Functions
###############################################################################

find_closest_backup() {
    local target_time=$1
    
    velero backup get -o json | \
        jq -r --arg target "$target_time" \
        '.items | 
         map(select(.status.phase=="Completed")) |
         map({name: .metadata.name, time: .metadata.creationTimestamp}) |
         sort_by(.time) |
         map(select(.time <= $target)) |
         last | .name'
}

cleanup_old_backups() {
    echo "üßπ Cleaning up old backups..."
    
    # Delete backups older than retention period
    velero backup get -o json | \
        jq -r --arg days "$BACKUP_RETENTION_DAYS" \
        '.items[] |
         select((now - (.metadata.creationTimestamp | fromdate)) > ($days | tonumber * 86400)) |
         .metadata.name' | \
    while read backup; do
        echo "Deleting old backup: $backup"
        velero backup delete "$backup" --confirm
    done
}

###############################################################################
# Main Setup
###############################################################################

setup_disaster_recovery() {
    echo "üíæ Setting up Disaster Recovery..."
    
    # Install tools
    install_velero
    configure_velero
    
    # Create backup schedules
    create_backup_schedules
    
    # Run initial backup
    backup_databases
    backup_secrets
    backup_application_data
    
    # Test DR procedures
    test_disaster_recovery
    
    echo "‚úÖ Disaster Recovery setup complete!"
    echo ""
    echo "üìä Configuration:"
    echo "   - Backup Storage: $BACKUP_STORAGE"
    echo "   - Retention: $BACKUP_RETENTION_DAYS days"
    echo "   - Schedule: $BACKUP_SCHEDULE (daily)"
    echo "   - Velero Version: $VELERO_VERSION"
}

# Run if called directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    setup_disaster_recovery
fi
