#!/usr/bin/env python3
"""
Server Init - Iteration 253: Outbox Pattern Platform
ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Outbox Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹

Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»:
- Outbox Table Management - ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ĞµĞ¹ outbox
- Event Capture - Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
- Reliable Publishing - Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
- Change Data Capture - Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
- Deduplication - Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
- Order Guarantee - Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°
- Cleanup & Archiving - Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
- Monitoring - Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
"""

import asyncio
import random
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Set
from enum import Enum
import uuid
import json
import hashlib


class OutboxEntryState(Enum):
    """Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ outbox"""
    PENDING = "pending"
    PROCESSING = "processing"
    PUBLISHED = "published"
    FAILED = "failed"
    ARCHIVED = "archived"


class AggregateType(Enum):
    """Ğ¢Ğ¸Ğ¿ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ°"""
    ORDER = "order"
    USER = "user"
    PAYMENT = "payment"
    INVENTORY = "inventory"
    NOTIFICATION = "notification"


class PublishResult(Enum):
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
    SUCCESS = "success"
    RETRY = "retry"
    FAILED = "failed"
    DUPLICATE = "duplicate"


@dataclass
class OutboxEntry:
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² outbox Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ"""
    entry_id: str
    
    # Aggregate
    aggregate_type: AggregateType = AggregateType.ORDER
    aggregate_id: str = ""
    
    # Event
    event_type: str = ""
    event_payload: Dict[str, Any] = field(default_factory=dict)
    
    # State
    state: OutboxEntryState = OutboxEntryState.PENDING
    
    # Publishing
    topic: str = ""
    partition_key: str = ""
    
    # Deduplication
    idempotency_key: str = ""
    payload_hash: str = ""
    
    # Ordering
    sequence_number: int = 0
    
    # Retry
    attempts: int = 0
    max_attempts: int = 5
    last_error: str = ""
    next_retry_at: Optional[datetime] = None
    
    # Metadata
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Time
    created_at: datetime = field(default_factory=datetime.now)
    processed_at: Optional[datetime] = None
    published_at: Optional[datetime] = None


@dataclass
class OutboxBatch:
    """Batch Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸"""
    batch_id: str
    
    # Entries
    entries: List[str] = field(default_factory=list)  # entry_ids
    
    # State
    started_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    # Stats
    total: int = 0
    published: int = 0
    failed: int = 0


@dataclass
class DeduplicationRecord:
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
    record_id: str
    idempotency_key: str
    payload_hash: str
    
    # Time
    created_at: datetime = field(default_factory=datetime.now)
    expires_at: datetime = field(default_factory=lambda: datetime.now() + timedelta(days=7))


@dataclass
class PublishMetrics:
    """ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
    topic: str
    
    # Counters
    total_published: int = 0
    total_failed: int = 0
    total_retried: int = 0
    total_duplicates: int = 0
    
    # Latency
    total_latency_ms: float = 0
    min_latency_ms: float = float('inf')
    max_latency_ms: float = 0
    
    # Time
    last_published_at: Optional[datetime] = None


@dataclass
class CleanupConfig:
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸"""
    retention_days: int = 30
    archive_enabled: bool = True
    batch_size: int = 1000
    
    # Schedule
    schedule_cron: str = "0 0 * * *"  # Daily at midnight


class OutboxManager:
    """ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Outbox Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°"""
    
    def __init__(self):
        self.entries: Dict[str, OutboxEntry] = {}
        self.batches: Dict[str, OutboxBatch] = {}
        self.dedup_records: Dict[str, DeduplicationRecord] = {}
        self.metrics: Dict[str, PublishMetrics] = {}
        self.archived_entries: List[OutboxEntry] = []
        
        # Sequence counters per aggregate
        self._sequences: Dict[str, int] = {}
        
        # Processing lock
        self._processing_entries: Set[str] = set()
        
        # Config
        self.cleanup_config = CleanupConfig()
        
    def capture_event(self, aggregate_type: AggregateType,
                     aggregate_id: str, event_type: str,
                     payload: Dict[str, Any],
                     topic: str = "",
                     idempotency_key: str = "") -> OutboxEntry:
        """Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ² outbox"""
        # Generate keys
        if not idempotency_key:
            idempotency_key = f"{aggregate_type.value}:{aggregate_id}:{event_type}:{uuid.uuid4().hex[:8]}"
            
        payload_hash = hashlib.md5(
            json.dumps(payload, sort_keys=True, default=str).encode()
        ).hexdigest()
        
        # Get sequence
        seq_key = f"{aggregate_type.value}:{aggregate_id}"
        self._sequences[seq_key] = self._sequences.get(seq_key, 0) + 1
        sequence = self._sequences[seq_key]
        
        # Create entry
        entry = OutboxEntry(
            entry_id=f"outbox_{uuid.uuid4().hex[:8]}",
            aggregate_type=aggregate_type,
            aggregate_id=aggregate_id,
            event_type=event_type,
            event_payload=payload,
            topic=topic or f"{aggregate_type.value}-events",
            partition_key=aggregate_id,
            idempotency_key=idempotency_key,
            payload_hash=payload_hash,
            sequence_number=sequence
        )
        
        self.entries[entry.entry_id] = entry
        
        # Initialize topic metrics
        if entry.topic not in self.metrics:
            self.metrics[entry.topic] = PublishMetrics(topic=entry.topic)
            
        return entry
        
    async def publish_entry(self, entry_id: str) -> PublishResult:
        """ĞŸÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸"""
        entry = self.entries.get(entry_id)
        if not entry:
            return PublishResult.FAILED
            
        if entry_id in self._processing_entries:
            return PublishResult.RETRY
            
        # Check deduplication
        if entry.idempotency_key in self.dedup_records:
            entry.state = OutboxEntryState.PUBLISHED
            self.metrics[entry.topic].total_duplicates += 1
            return PublishResult.DUPLICATE
            
        # Mark as processing
        self._processing_entries.add(entry_id)
        entry.state = OutboxEntryState.PROCESSING
        entry.attempts += 1
        
        start_time = datetime.now()
        
        try:
            # Simulate publishing
            await asyncio.sleep(random.uniform(0.01, 0.1))
            
            if random.random() < 0.9:  # 90% success rate
                # Success
                entry.state = OutboxEntryState.PUBLISHED
                entry.published_at = datetime.now()
                entry.processed_at = datetime.now()
                
                # Record for deduplication
                self.dedup_records[entry.idempotency_key] = DeduplicationRecord(
                    record_id=f"dedup_{uuid.uuid4().hex[:8]}",
                    idempotency_key=entry.idempotency_key,
                    payload_hash=entry.payload_hash
                )
                
                # Update metrics
                metrics = self.metrics[entry.topic]
                latency = (datetime.now() - start_time).total_seconds() * 1000
                metrics.total_published += 1
                metrics.total_latency_ms += latency
                metrics.min_latency_ms = min(metrics.min_latency_ms, latency)
                metrics.max_latency_ms = max(metrics.max_latency_ms, latency)
                metrics.last_published_at = datetime.now()
                
                return PublishResult.SUCCESS
            else:
                raise Exception("Simulated publish failure")
                
        except Exception as e:
            entry.last_error = str(e)
            
            if entry.attempts >= entry.max_attempts:
                entry.state = OutboxEntryState.FAILED
                self.metrics[entry.topic].total_failed += 1
                return PublishResult.FAILED
            else:
                entry.state = OutboxEntryState.PENDING
                entry.next_retry_at = datetime.now() + timedelta(
                    seconds=2 ** entry.attempts  # Exponential backoff
                )
                self.metrics[entry.topic].total_retried += 1
                return PublishResult.RETRY
                
        finally:
            self._processing_entries.discard(entry_id)
            
    async def process_pending(self, batch_size: int = 100) -> OutboxBatch:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° pending Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"""
        batch = OutboxBatch(
            batch_id=f"batch_{uuid.uuid4().hex[:8]}",
            started_at=datetime.now()
        )
        
        # Get pending entries
        pending = [
            e for e in self.entries.values()
            if e.state == OutboxEntryState.PENDING
            and e.entry_id not in self._processing_entries
            and (e.next_retry_at is None or e.next_retry_at <= datetime.now())
        ]
        
        # Sort by sequence for ordering guarantee
        pending.sort(key=lambda e: (e.aggregate_id, e.sequence_number))
        
        # Process batch
        for entry in pending[:batch_size]:
            batch.entries.append(entry.entry_id)
            batch.total += 1
            
            result = await self.publish_entry(entry.entry_id)
            
            if result == PublishResult.SUCCESS:
                batch.published += 1
            elif result == PublishResult.FAILED:
                batch.failed += 1
                
        batch.completed_at = datetime.now()
        self.batches[batch.batch_id] = batch
        
        return batch
        
    async def cleanup_old_entries(self) -> Dict[str, int]:
        """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"""
        cutoff = datetime.now() - timedelta(days=self.cleanup_config.retention_days)
        
        archived = 0
        deleted = 0
        
        entries_to_process = [
            e for e in self.entries.values()
            if e.state == OutboxEntryState.PUBLISHED
            and e.published_at and e.published_at < cutoff
        ]
        
        for entry in entries_to_process:
            if self.cleanup_config.archive_enabled:
                entry.state = OutboxEntryState.ARCHIVED
                self.archived_entries.append(entry)
                archived += 1
            else:
                deleted += 1
                
            del self.entries[entry.entry_id]
            
        # Cleanup dedup records
        dedup_expired = [
            key for key, record in self.dedup_records.items()
            if record.expires_at < datetime.now()
        ]
        
        for key in dedup_expired:
            del self.dedup_records[key]
            
        return {
            "archived": archived,
            "deleted": deleted,
            "dedup_cleaned": len(dedup_expired)
        }
        
    def get_pending_count(self) -> int:
        """ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ pending Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"""
        return sum(1 for e in self.entries.values() if e.state == OutboxEntryState.PENDING)
        
    def get_failed_entries(self) -> List[OutboxEntry]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ failed Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"""
        return [e for e in self.entries.values() if e.state == OutboxEntryState.FAILED]
        
    def get_entries_by_aggregate(self, aggregate_type: AggregateType,
                                aggregate_id: str) -> List[OutboxEntry]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¿Ğ¾ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ñƒ"""
        return [
            e for e in self.entries.values()
            if e.aggregate_type == aggregate_type
            and e.aggregate_id == aggregate_id
        ]
        
    def get_topic_metrics(self, topic: str) -> Optional[PublishMetrics]:
        """ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ğ¿Ğ¸ĞºÑƒ"""
        return self.metrics.get(topic)
        
    def get_statistics(self) -> Dict[str, Any]:
        """ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°"""
        state_counts: Dict[OutboxEntryState, int] = {}
        for entry in self.entries.values():
            state_counts[entry.state] = state_counts.get(entry.state, 0) + 1
            
        total_published = sum(m.total_published for m in self.metrics.values())
        total_failed = sum(m.total_failed for m in self.metrics.values())
        total_latency = sum(m.total_latency_ms for m in self.metrics.values())
        
        avg_latency = (total_latency / total_published) if total_published > 0 else 0
        
        return {
            "entries_total": len(self.entries),
            "entries_pending": state_counts.get(OutboxEntryState.PENDING, 0),
            "entries_processing": state_counts.get(OutboxEntryState.PROCESSING, 0),
            "entries_published": state_counts.get(OutboxEntryState.PUBLISHED, 0),
            "entries_failed": state_counts.get(OutboxEntryState.FAILED, 0),
            "batches_processed": len(self.batches),
            "dedup_records": len(self.dedup_records),
            "archived_entries": len(self.archived_entries),
            "total_published": total_published,
            "total_failed": total_failed,
            "avg_latency_ms": avg_latency,
            "topics_count": len(self.metrics)
        }


# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
async def main():
    print("=" * 60)
    print("Server Init - Iteration 253: Outbox Pattern Platform")
    print("=" * 60)
    
    manager = OutboxManager()
    print("âœ“ Outbox Manager created")
    
    # Capture events
    print("\nğŸ“¤ Capturing Events to Outbox...")
    
    events_to_capture = [
        (AggregateType.ORDER, "ORD-001", "order.created", {"amount": 99.99, "items": 3}),
        (AggregateType.ORDER, "ORD-001", "order.paid", {"payment_id": "PAY-001"}),
        (AggregateType.ORDER, "ORD-001", "order.shipped", {"tracking": "TRK123"}),
        (AggregateType.ORDER, "ORD-002", "order.created", {"amount": 149.99, "items": 5}),
        (AggregateType.USER, "USR-001", "user.created", {"email": "alice@example.com"}),
        (AggregateType.USER, "USR-001", "user.verified", {"verified": True}),
        (AggregateType.PAYMENT, "PAY-001", "payment.captured", {"amount": 99.99}),
        (AggregateType.INVENTORY, "INV-001", "inventory.reserved", {"product_id": "P001", "qty": 2}),
    ]
    
    entries = []
    for agg_type, agg_id, event_type, payload in events_to_capture:
        entry = manager.capture_event(agg_type, agg_id, event_type, payload)
        entries.append(entry)
        print(f"  ğŸ“¤ {event_type} -> {entry.topic} (seq: {entry.sequence_number})")
        
    # Process pending entries
    print("\nğŸ”„ Processing Pending Entries...")
    
    batch = await manager.process_pending(batch_size=10)
    print(f"  ğŸ“¦ Batch {batch.batch_id[:12]}...")
    print(f"     Total: {batch.total}, Published: {batch.published}, Failed: {batch.failed}")
    
    # Process remaining
    while manager.get_pending_count() > 0:
        batch = await manager.process_pending(batch_size=10)
        if batch.total == 0:
            break
            
    # Display entries
    print("\nğŸ“‹ Outbox Entries:")
    
    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Entry            â”‚ Aggregate     â”‚ Event Type      â”‚ State     â”‚ Attempts â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for entry in list(manager.entries.values())[:8]:
        entry_id = entry.entry_id[:16].ljust(16)
        agg = f"{entry.aggregate_type.value[:4]}:{entry.aggregate_id[-4:]}"[:13].ljust(13)
        event = entry.event_type[:15].ljust(15)
        state = entry.state.value[:9].ljust(9)
        attempts = str(entry.attempts)[:8].ljust(8)
        
        print(f"  â”‚ {entry_id} â”‚ {agg} â”‚ {event} â”‚ {state} â”‚ {attempts} â”‚")
        
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Display by aggregate
    print("\nğŸ“Š Entries by Aggregate (ORD-001):")
    
    ord_entries = manager.get_entries_by_aggregate(AggregateType.ORDER, "ORD-001")
    for entry in ord_entries:
        status_icon = "âœ“" if entry.state == OutboxEntryState.PUBLISHED else "â—‹"
        print(f"  {status_icon} [{entry.sequence_number}] {entry.event_type}: {entry.state.value}")
        
    # Topic metrics
    print("\nğŸ“Š Topic Metrics:")
    
    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Topic               â”‚ Publishedâ”‚ Failed   â”‚ Retried  â”‚ Avg(ms)  â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for topic, metrics in manager.metrics.items():
        topic_name = topic[:19].ljust(19)
        published = str(metrics.total_published)[:8].ljust(8)
        failed = str(metrics.total_failed)[:8].ljust(8)
        retried = str(metrics.total_retried)[:8].ljust(8)
        avg_lat = f"{metrics.total_latency_ms / max(1, metrics.total_published):.1f}"[:8].ljust(8)
        
        print(f"  â”‚ {topic_name} â”‚ {published} â”‚ {failed} â”‚ {retried} â”‚ {avg_lat} â”‚")
        
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Deduplication test
    print("\nğŸ”’ Testing Deduplication...")
    
    # Try to capture same event again
    dup_entry = manager.capture_event(
        AggregateType.ORDER, "ORD-001", "order.created",
        {"amount": 99.99, "items": 3}
    )
    
    result = await manager.publish_entry(dup_entry.entry_id)
    if result == PublishResult.DUPLICATE:
        print(f"  ğŸ”’ Duplicate detected for {dup_entry.event_type}")
    else:
        print(f"  ğŸ“¤ Published (result: {result.value})")
        
    # Display dedup records
    print(f"\nğŸ” Deduplication Records: {len(manager.dedup_records)}")
    
    for key, record in list(manager.dedup_records.items())[:3]:
        print(f"  ğŸ” {record.idempotency_key[:40]}...")
        
    # Failed entries
    print("\nâŒ Failed Entries:")
    
    failed = manager.get_failed_entries()
    if failed:
        for entry in failed[:3]:
            print(f"  âŒ {entry.event_type}: {entry.last_error}")
    else:
        print("  No failed entries")
        
    # Cleanup simulation
    print("\nğŸ§¹ Cleanup Configuration:")
    
    config = manager.cleanup_config
    print(f"  Retention: {config.retention_days} days")
    print(f"  Archive: {'enabled' if config.archive_enabled else 'disabled'}")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  Schedule: {config.schedule_cron}")
    
    # State distribution
    print("\nğŸ“Š Entry State Distribution:")
    
    state_counts: Dict[OutboxEntryState, int] = {}
    for entry in manager.entries.values():
        state_counts[entry.state] = state_counts.get(entry.state, 0) + 1
        
    for state in OutboxEntryState:
        count = state_counts.get(state, 0)
        bar = "â–ˆ" * min(count, 10) + "â–‘" * (10 - min(count, 10))
        icon = {
            OutboxEntryState.PENDING: "â—‹",
            OutboxEntryState.PROCESSING: "â—",
            OutboxEntryState.PUBLISHED: "âœ“",
            OutboxEntryState.FAILED: "âœ—",
            OutboxEntryState.ARCHIVED: "ğŸ“¦"
        }.get(state, "?")
        print(f"  {icon} {state.value:12s} [{bar}] {count}")
        
    # Statistics
    print("\nğŸ“Š Outbox Statistics:")
    
    stats = manager.get_statistics()
    
    print(f"\n  Entries Total: {stats['entries_total']}")
    print(f"  Pending: {stats['entries_pending']}")
    print(f"  Published: {stats['entries_published']}")
    print(f"  Failed: {stats['entries_failed']}")
    
    print(f"\n  Batches Processed: {stats['batches_processed']}")
    print(f"  Dedup Records: {stats['dedup_records']}")
    print(f"  Topics: {stats['topics_count']}")
    
    print(f"\n  Total Published: {stats['total_published']}")
    print(f"  Avg Latency: {stats['avg_latency_ms']:.1f}ms")
    
    # Dashboard
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                     Outbox Pattern Dashboard                        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Total Entries:                 {stats['entries_total']:>12}                        â”‚")
    print(f"â”‚ Pending:                       {stats['entries_pending']:>12}                        â”‚")
    print(f"â”‚ Published:                     {stats['entries_published']:>12}                        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Total Published:               {stats['total_published']:>12}                        â”‚")
    print(f"â”‚ Avg Latency:                   {stats['avg_latency_ms']:>10.1f}ms                        â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\n" + "=" * 60)
    print("Outbox Pattern Platform initialized!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
