#!/usr/bin/env python3
"""
Server Init - Iteration 276: Log Aggregation Platform
–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ª–æ–≥–æ–≤

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- Log Collection - —Å–±–æ—Ä –ª–æ–≥–æ–≤
- Log Parsing - –ø–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–æ–≤
- Log Indexing - –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ª–æ–≥–æ–≤
- Log Search - –ø–æ–∏—Å–∫ –ø–æ –ª–æ–≥–∞–º
- Log Streaming - –ø–æ—Ç–æ–∫–æ–≤–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –ª–æ–≥–æ–≤
- Log Retention - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
- Log Alerting - –æ–ø–æ–≤–µ—â–µ–Ω–∏—è –ø–æ –ª–æ–≥–∞–º
- Log Analytics - –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ª–æ–≥–æ–≤
"""

import asyncio
import random
import re
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Pattern
from enum import Enum
import uuid


class LogLevel(Enum):
    """–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–æ–≤"""
    TRACE = 0
    DEBUG = 1
    INFO = 2
    WARNING = 3
    ERROR = 4
    CRITICAL = 5


class LogFormat(Enum):
    """–§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤"""
    JSON = "json"
    PLAINTEXT = "plaintext"
    SYSLOG = "syslog"
    APACHE = "apache"
    NGINX = "nginx"
    CUSTOM = "custom"


class LogSourceType(Enum):
    """–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ª–æ–≥–æ–≤"""
    FILE = "file"
    DOCKER = "docker"
    KUBERNETES = "kubernetes"
    SYSLOG = "syslog"
    HTTP = "http"
    TCP = "tcp"
    UDP = "udp"


class AlertCondition(Enum):
    """–£—Å–ª–æ–≤–∏–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
    CONTAINS = "contains"
    MATCHES_REGEX = "matches_regex"
    COUNT_EXCEEDS = "count_exceeds"
    RATE_EXCEEDS = "rate_exceeds"


@dataclass
class LogEntry:
    """–ó–∞–ø–∏—Å—å –ª–æ–≥–∞"""
    log_id: str
    
    # Timestamp
    timestamp: datetime = field(default_factory=datetime.now)
    
    # Level
    level: LogLevel = LogLevel.INFO
    
    # Message
    message: str = ""
    raw_message: str = ""
    
    # Source
    source: str = ""
    host: str = ""
    
    # Application
    application: str = ""
    environment: str = ""
    
    # Context
    trace_id: Optional[str] = None
    span_id: Optional[str] = None
    
    # Extra fields
    fields: Dict[str, Any] = field(default_factory=dict)
    
    # Tags
    tags: List[str] = field(default_factory=list)
    
    # Indexed
    indexed: bool = False


@dataclass
class LogSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –ª–æ–≥–æ–≤"""
    source_id: str
    name: str
    
    # Type
    source_type: LogSourceType = LogSourceType.FILE
    
    # Path/URL
    path: str = ""
    
    # Format
    log_format: LogFormat = LogFormat.JSON
    
    # Parser
    parser_pattern: Optional[str] = None
    
    # Labels
    labels: Dict[str, str] = field(default_factory=dict)
    
    # State
    active: bool = True
    logs_collected: int = 0
    last_collection: Optional[datetime] = None


@dataclass
class LogParser:
    """–ü–∞—Ä—Å–µ—Ä –ª–æ–≥–æ–≤"""
    parser_id: str
    name: str
    
    # Format
    log_format: LogFormat = LogFormat.JSON
    
    # Pattern
    pattern: Optional[str] = None
    compiled_pattern: Optional[Pattern] = None
    
    # Field mappings
    field_mappings: Dict[str, str] = field(default_factory=dict)
    
    # Timestamp format
    timestamp_format: str = "%Y-%m-%dT%H:%M:%S"


@dataclass
class LogIndex:
    """–ò–Ω–¥–µ–∫—Å –ª–æ–≥–æ–≤"""
    index_id: str
    name: str
    
    # Shards
    shards: int = 1
    replicas: int = 0
    
    # Retention
    retention_days: int = 30
    
    # Stats
    doc_count: int = 0
    size_bytes: int = 0
    
    # Created
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class LogStream:
    """–ü–æ—Ç–æ–∫ –ª–æ–≥–æ–≤"""
    stream_id: str
    name: str
    
    # Filter
    filter_query: str = "*"
    level_filter: Optional[LogLevel] = None
    
    # Sources
    source_filters: List[str] = field(default_factory=list)
    
    # Active subscribers
    subscribers: int = 0
    
    # State
    active: bool = True


@dataclass
class LogAlert:
    """–û–ø–æ–≤–µ—â–µ–Ω–∏–µ –ø–æ –ª–æ–≥–∞–º"""
    alert_id: str
    name: str
    
    # Condition
    condition: AlertCondition = AlertCondition.CONTAINS
    condition_value: str = ""
    threshold: int = 1
    
    # Window
    window_minutes: int = 5
    
    # Actions
    notify_channels: List[str] = field(default_factory=list)
    
    # State
    active: bool = True
    triggered: bool = False
    trigger_count: int = 0
    last_triggered: Optional[datetime] = None


@dataclass
class LogQuery:
    """–ó–∞–ø—Ä–æ—Å –ª–æ–≥–æ–≤"""
    query_id: str
    
    # Query
    query_string: str = "*"
    
    # Filters
    level: Optional[LogLevel] = None
    source: Optional[str] = None
    application: Optional[str] = None
    
    # Time range
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    # Pagination
    offset: int = 0
    limit: int = 100


@dataclass
class LogAnalytics:
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ª–æ–≥–æ–≤"""
    analytics_id: str
    name: str
    
    # Aggregation
    group_by: List[str] = field(default_factory=list)  # fields to group by
    
    # Metrics
    count: int = 0
    error_count: int = 0
    
    # Time series
    time_buckets: Dict[str, int] = field(default_factory=dict)


@dataclass
class RetentionPolicy:
    """–ü–æ–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
    policy_id: str
    name: str
    
    # Duration
    retention_days: int = 30
    
    # Actions
    delete_after_days: int = 90
    archive_after_days: int = 30
    
    # Filters
    level_filter: Optional[LogLevel] = None
    source_filter: Optional[str] = None
    
    # State
    active: bool = True


class LogAggregationManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.logs: List[LogEntry] = []
        self.sources: Dict[str, LogSource] = {}
        self.parsers: Dict[str, LogParser] = {}
        self.indexes: Dict[str, LogIndex] = {}
        self.streams: Dict[str, LogStream] = {}
        self.alerts: Dict[str, LogAlert] = {}
        self.policies: Dict[str, RetentionPolicy] = {}
        
        # Initialize default parser
        self._init_default_parsers()
        
    def _init_default_parsers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        # JSON parser
        self.parsers["json"] = LogParser(
            parser_id="parser_json",
            name="json",
            log_format=LogFormat.JSON
        )
        
        # Apache parser
        apache_pattern = r'^(?P<host>\S+) \S+ \S+ \[(?P<timestamp>[^\]]+)\] "(?P<method>\S+) (?P<path>\S+) (?P<protocol>[^"]*)" (?P<status>\d+) (?P<size>\d+)'
        self.parsers["apache"] = LogParser(
            parser_id="parser_apache",
            name="apache",
            log_format=LogFormat.APACHE,
            pattern=apache_pattern,
            compiled_pattern=re.compile(apache_pattern)
        )
        
    def add_source(self, name: str,
                  source_type: LogSourceType,
                  path: str,
                  log_format: LogFormat = LogFormat.JSON,
                  labels: Dict[str, str] = None) -> LogSource:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        source = LogSource(
            source_id=f"source_{uuid.uuid4().hex[:8]}",
            name=name,
            source_type=source_type,
            path=path,
            log_format=log_format,
            labels=labels or {}
        )
        
        self.sources[name] = source
        return source
        
    def create_parser(self, name: str,
                     log_format: LogFormat,
                     pattern: str = None,
                     field_mappings: Dict[str, str] = None) -> LogParser:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
        parser = LogParser(
            parser_id=f"parser_{uuid.uuid4().hex[:8]}",
            name=name,
            log_format=log_format,
            pattern=pattern,
            field_mappings=field_mappings or {}
        )
        
        if pattern:
            parser.compiled_pattern = re.compile(pattern)
            
        self.parsers[name] = parser
        return parser
        
    def parse_log(self, raw_message: str,
                 parser_name: str = "json") -> Optional[LogEntry]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–∞"""
        parser = self.parsers.get(parser_name)
        if not parser:
            return None
            
        entry = LogEntry(
            log_id=f"log_{uuid.uuid4().hex[:8]}",
            raw_message=raw_message
        )
        
        if parser.log_format == LogFormat.JSON:
            try:
                import json
                data = json.loads(raw_message)
                
                entry.message = data.get("message", data.get("msg", raw_message))
                entry.level = self._parse_level(data.get("level", "info"))
                entry.timestamp = self._parse_timestamp(data.get("timestamp", data.get("time")))
                entry.source = data.get("source", data.get("logger", ""))
                entry.application = data.get("application", data.get("app", ""))
                
                # Extract extra fields
                for key, value in data.items():
                    if key not in ["message", "msg", "level", "timestamp", "time", "source", "logger", "application", "app"]:
                        entry.fields[key] = value
                        
            except:
                entry.message = raw_message
                
        elif parser.compiled_pattern:
            match = parser.compiled_pattern.match(raw_message)
            if match:
                groups = match.groupdict()
                entry.message = groups.get("message", raw_message)
                entry.level = self._parse_level(groups.get("level", "info"))
                entry.timestamp = self._parse_timestamp(groups.get("timestamp"))
                entry.host = groups.get("host", "")
                entry.fields = {k: v for k, v in groups.items() if k not in ["message", "level", "timestamp", "host"]}
            else:
                entry.message = raw_message
        else:
            entry.message = raw_message
            
        return entry
        
    def _parse_level(self, level_str: str) -> LogLevel:
        """–ü–∞—Ä—Å–∏–Ω–≥ —É—Ä–æ–≤–Ω—è"""
        level_str = level_str.lower()
        level_map = {
            "trace": LogLevel.TRACE,
            "debug": LogLevel.DEBUG,
            "info": LogLevel.INFO,
            "warn": LogLevel.WARNING,
            "warning": LogLevel.WARNING,
            "error": LogLevel.ERROR,
            "err": LogLevel.ERROR,
            "critical": LogLevel.CRITICAL,
            "fatal": LogLevel.CRITICAL
        }
        return level_map.get(level_str, LogLevel.INFO)
        
    def _parse_timestamp(self, ts) -> datetime:
        """–ü–∞—Ä—Å–∏–Ω–≥ timestamp"""
        if ts is None:
            return datetime.now()
        if isinstance(ts, datetime):
            return ts
        if isinstance(ts, (int, float)):
            return datetime.fromtimestamp(ts)
        try:
            return datetime.fromisoformat(str(ts).replace("Z", "+00:00"))
        except:
            return datetime.now()
            
    def ingest_log(self, entry: LogEntry,
                  source_name: str = None) -> LogEntry:
        """–ü—Ä–∏–µ–º –ª–æ–≥–∞"""
        if source_name:
            entry.source = source_name
            if source_name in self.sources:
                source = self.sources[source_name]
                source.logs_collected += 1
                source.last_collection = datetime.now()
                
        self.logs.append(entry)
        
        # Check alerts
        self._check_alerts(entry)
        
        return entry
        
    def create_index(self, name: str,
                    shards: int = 1,
                    retention_days: int = 30) -> LogIndex:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        index = LogIndex(
            index_id=f"index_{uuid.uuid4().hex[:8]}",
            name=name,
            shards=shards,
            retention_days=retention_days
        )
        
        self.indexes[name] = index
        return index
        
    def index_log(self, entry: LogEntry,
                 index_name: str) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ª–æ–≥–∞"""
        index = self.indexes.get(index_name)
        if not index:
            return False
            
        entry.indexed = True
        index.doc_count += 1
        index.size_bytes += len(entry.raw_message or entry.message)
        
        return True
        
    def search(self, query: LogQuery) -> List[LogEntry]:
        """–ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤"""
        results = []
        
        for entry in self.logs:
            if not self._matches_query(entry, query):
                continue
            results.append(entry)
            
        # Sort by timestamp desc
        results.sort(key=lambda x: x.timestamp, reverse=True)
        
        # Apply pagination
        return results[query.offset:query.offset + query.limit]
        
    def _matches_query(self, entry: LogEntry, query: LogQuery) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–ø—Ä–æ—Å—É"""
        # Query string
        if query.query_string and query.query_string != "*":
            if query.query_string.lower() not in entry.message.lower():
                return False
                
        # Level filter
        if query.level and entry.level.value < query.level.value:
            return False
            
        # Source filter
        if query.source and entry.source != query.source:
            return False
            
        # Application filter
        if query.application and entry.application != query.application:
            return False
            
        # Time range
        if query.start_time and entry.timestamp < query.start_time:
            return False
        if query.end_time and entry.timestamp > query.end_time:
            return False
            
        return True
        
    def create_stream(self, name: str,
                     filter_query: str = "*",
                     level_filter: LogLevel = None) -> LogStream:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞"""
        stream = LogStream(
            stream_id=f"stream_{uuid.uuid4().hex[:8]}",
            name=name,
            filter_query=filter_query,
            level_filter=level_filter
        )
        
        self.streams[name] = stream
        return stream
        
    def create_alert(self, name: str,
                    condition: AlertCondition,
                    condition_value: str,
                    threshold: int = 1,
                    notify_channels: List[str] = None) -> LogAlert:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        alert = LogAlert(
            alert_id=f"alert_{uuid.uuid4().hex[:8]}",
            name=name,
            condition=condition,
            condition_value=condition_value,
            threshold=threshold,
            notify_channels=notify_channels or []
        )
        
        self.alerts[name] = alert
        return alert
        
    def _check_alerts(self, entry: LogEntry):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π"""
        for alert in self.alerts.values():
            if not alert.active:
                continue
                
            triggered = False
            
            if alert.condition == AlertCondition.CONTAINS:
                if alert.condition_value.lower() in entry.message.lower():
                    triggered = True
                    
            elif alert.condition == AlertCondition.MATCHES_REGEX:
                try:
                    if re.search(alert.condition_value, entry.message):
                        triggered = True
                except:
                    pass
                    
            if triggered:
                alert.trigger_count += 1
                if alert.trigger_count >= alert.threshold:
                    alert.triggered = True
                    alert.last_triggered = datetime.now()
                    
    def set_retention_policy(self, name: str,
                            retention_days: int,
                            level_filter: LogLevel = None) -> RetentionPolicy:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        policy = RetentionPolicy(
            policy_id=f"policy_{uuid.uuid4().hex[:8]}",
            name=name,
            retention_days=retention_days,
            level_filter=level_filter
        )
        
        self.policies[name] = policy
        return policy
        
    def apply_retention(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        cutoff = datetime.now()
        deleted = 0
        
        for policy in self.policies.values():
            if not policy.active:
                continue
                
            policy_cutoff = cutoff - timedelta(days=policy.retention_days)
            
            new_logs = []
            for entry in self.logs:
                keep = True
                
                if entry.timestamp < policy_cutoff:
                    if policy.level_filter:
                        if entry.level.value <= policy.level_filter.value:
                            keep = False
                    else:
                        keep = False
                        
                if keep:
                    new_logs.append(entry)
                else:
                    deleted += 1
                    
            self.logs = new_logs
            
        return deleted
        
    def get_analytics(self, group_by: List[str] = None,
                     start_time: datetime = None,
                     end_time: datetime = None) -> LogAnalytics:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        analytics = LogAnalytics(
            analytics_id=f"analytics_{uuid.uuid4().hex[:8]}",
            name="log_analytics",
            group_by=group_by or []
        )
        
        for entry in self.logs:
            if start_time and entry.timestamp < start_time:
                continue
            if end_time and entry.timestamp > end_time:
                continue
                
            analytics.count += 1
            if entry.level.value >= LogLevel.ERROR.value:
                analytics.error_count += 1
                
            # Time bucket
            bucket = entry.timestamp.strftime("%Y-%m-%d %H:00")
            analytics.time_buckets[bucket] = analytics.time_buckets.get(bucket, 0) + 1
            
        return analytics
        
    def get_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        level_counts = {}
        for entry in self.logs:
            level_counts[entry.level.name] = level_counts.get(entry.level.name, 0) + 1
            
        return {
            "total_logs": len(self.logs),
            "sources": len(self.sources),
            "parsers": len(self.parsers),
            "indexes": len(self.indexes),
            "streams": len(self.streams),
            "alerts": len(self.alerts),
            "policies": len(self.policies),
            "level_distribution": level_counts
        }


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
async def main():
    print("=" * 60)
    print("Server Init - Iteration 276: Log Aggregation Platform")
    print("=" * 60)
    
    manager = LogAggregationManager()
    print("‚úì Log Aggregation Manager created")
    
    # Add sources
    print("\nüì• Adding Log Sources...")
    
    sources_config = [
        ("nginx-access", LogSourceType.FILE, "/var/log/nginx/access.log", LogFormat.NGINX),
        ("app-logs", LogSourceType.DOCKER, "container://app", LogFormat.JSON),
        ("kubernetes", LogSourceType.KUBERNETES, "namespace/default", LogFormat.JSON),
        ("syslog", LogSourceType.SYSLOG, "udp://0.0.0.0:514", LogFormat.SYSLOG),
    ]
    
    for name, stype, path, fmt in sources_config:
        source = manager.add_source(name, stype, path, fmt)
        print(f"  üì• {name}: {stype.value}")
        
    # Create indexes
    print("\nüìá Creating Indexes...")
    
    indexes_config = [
        ("logs-daily", 3, 7),
        ("logs-weekly", 1, 30),
        ("errors", 2, 90),
    ]
    
    for name, shards, retention in indexes_config:
        index = manager.create_index(name, shards, retention)
        print(f"  üìá {name}: {shards} shards, {retention}d retention")
        
    # Create streams
    print("\nüåä Creating Log Streams...")
    
    error_stream = manager.create_stream("errors", "error OR exception", LogLevel.ERROR)
    print(f"  üåä {error_stream.name}: level >= ERROR")
    
    auth_stream = manager.create_stream("authentication", "login OR logout OR auth")
    print(f"  üåä {auth_stream.name}: auth events")
    
    # Create alerts
    print("\nüö® Creating Alerts...")
    
    alerts_config = [
        ("high-error-rate", AlertCondition.MATCHES_REGEX, r"(error|exception|failed)", 10),
        ("security-alert", AlertCondition.CONTAINS, "unauthorized", 1),
        ("database-error", AlertCondition.CONTAINS, "database connection failed", 3),
    ]
    
    for name, condition, value, threshold in alerts_config:
        alert = manager.create_alert(name, condition, value, threshold, ["slack", "email"])
        print(f"  üö® {name}: {condition.value} threshold={threshold}")
        
    # Set retention policies
    print("\nüìã Setting Retention Policies...")
    
    manager.set_retention_policy("debug-short", 7, LogLevel.DEBUG)
    manager.set_retention_policy("info-medium", 30, LogLevel.INFO)
    manager.set_retention_policy("error-long", 90, LogLevel.ERROR)
    print("  Policies: debug=7d, info=30d, error=90d")
    
    # Ingest logs
    print("\nüìù Ingesting Logs...")
    
    applications = ["api-gateway", "user-service", "order-service", "payment-service"]
    log_templates = [
        ('{"level": "info", "message": "Request received from %s", "application": "%s", "timestamp": "%s"}', LogLevel.INFO),
        ('{"level": "debug", "message": "Processing order %d", "application": "%s", "timestamp": "%s"}', LogLevel.DEBUG),
        ('{"level": "warning", "message": "High latency detected: %dms", "application": "%s", "timestamp": "%s"}', LogLevel.WARNING),
        ('{"level": "error", "message": "Database connection failed: %s", "application": "%s", "timestamp": "%s"}', LogLevel.ERROR),
        ('{"level": "info", "message": "User login successful: %s", "application": "%s", "timestamp": "%s"}', LogLevel.INFO),
        ('{"level": "error", "message": "Payment processing exception: %s", "application": "%s", "timestamp": "%s"}', LogLevel.ERROR),
        ('{"level": "critical", "message": "Service unavailable: unauthorized access", "application": "%s", "timestamp": "%s"}', LogLevel.CRITICAL),
    ]
    
    for i in range(50):
        template, level = random.choice(log_templates)
        app = random.choice(applications)
        ts = (datetime.now() - timedelta(minutes=random.randint(0, 60))).isoformat()
        
        if "%d" in template:
            raw = template % (random.randint(100, 9999), app, ts)
        elif template.count("%s") == 3:
            raw = template % (f"192.168.1.{random.randint(1, 254)}", app, ts)
        else:
            raw = template % (app, ts)
            
        entry = manager.parse_log(raw, "json")
        if entry:
            manager.ingest_log(entry, "app-logs")
            manager.index_log(entry, "logs-daily")
            
    print(f"  Ingested {len(manager.logs)} logs")
    
    # Search logs
    print("\nüîé Searching Logs...")
    
    # Search errors
    query = LogQuery(
        query_id="q1",
        query_string="error",
        level=LogLevel.ERROR,
        limit=5
    )
    
    results = manager.search(query)
    print(f"\n  Error logs: {len(results)} results")
    
    for entry in results[:3]:
        level_icon = {
            LogLevel.DEBUG: "üîç",
            LogLevel.INFO: "‚ÑπÔ∏è",
            LogLevel.WARNING: "‚ö†Ô∏è",
            LogLevel.ERROR: "‚ùå",
            LogLevel.CRITICAL: "üö®"
        }.get(entry.level, "‚ùì")
        
        msg = entry.message[:50] + "..." if len(entry.message) > 50 else entry.message
        print(f"    {level_icon} [{entry.application}] {msg}")
        
    # Get analytics
    print("\nüìä Log Analytics...")
    
    analytics = manager.get_analytics()
    print(f"  Total logs: {analytics.count}")
    print(f"  Error logs: {analytics.error_count}")
    print(f"  Error rate: {analytics.error_count / analytics.count * 100:.1f}%")
    
    # Display level distribution
    print("\nüìä Level Distribution:")
    
    stats = manager.get_statistics()
    level_dist = stats["level_distribution"]
    
    total = sum(level_dist.values())
    for level, count in sorted(level_dist.items(), key=lambda x: LogLevel[x[0]].value):
        pct = count / total * 100
        bar = "‚ñà" * int(pct / 5) + "‚ñë" * (20 - int(pct / 5))
        print(f"  {level:10s} [{bar}] {count:4d} ({pct:5.1f}%)")
        
    # Display time buckets
    print("\nüìà Log Volume by Hour:")
    
    sorted_buckets = sorted(analytics.time_buckets.items())
    if sorted_buckets:
        max_count = max(analytics.time_buckets.values())
        for bucket, count in sorted_buckets[-5:]:
            bar_len = int(count / max_count * 20)
            bar = "‚ñà" * bar_len
            print(f"  {bucket[-5:]}: {bar} {count}")
            
    # Display sources
    print("\nüì• Source Statistics:")
    
    print("\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("  ‚îÇ Source             ‚îÇ Type        ‚îÇ Logs        ‚îÇ Last Collection      ‚îÇ")
    print("  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    
    for source in manager.sources.values():
        name = source.name[:18].ljust(18)
        stype = source.source_type.value[:11].ljust(11)
        logs = str(source.logs_collected)[:11].ljust(11)
        last = source.last_collection.strftime("%Y-%m-%d %H:%M") if source.last_collection else "Never"
        last = last[:20].ljust(20)
        
        print(f"  ‚îÇ {name} ‚îÇ {stype} ‚îÇ {logs} ‚îÇ {last} ‚îÇ")
        
    print("  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    
    # Display indexes
    print("\nüìá Index Statistics:")
    
    print("\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("  ‚îÇ Index              ‚îÇ Shards  ‚îÇ Documents   ‚îÇ Size        ‚îÇ")
    print("  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    
    for index in manager.indexes.values():
        name = index.name[:18].ljust(18)
        shards = str(index.shards)[:7].ljust(7)
        docs = str(index.doc_count)[:11].ljust(11)
        size = f"{index.size_bytes / 1024:.1f}KB"[:11].ljust(11)
        
        print(f"  ‚îÇ {name} ‚îÇ {shards} ‚îÇ {docs} ‚îÇ {size} ‚îÇ")
        
    print("  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    
    # Display alerts
    print("\nüö® Alert Status:")
    
    for alert in manager.alerts.values():
        status = "üî• FIRING" if alert.triggered else "‚úÖ OK"
        print(f"  {alert.name}: {status} (triggers: {alert.trigger_count}/{alert.threshold})")
        
    # Display streams
    print("\nüåä Stream Status:")
    
    for stream in manager.streams.values():
        status = "üü¢ Active" if stream.active else "üî¥ Inactive"
        level_str = f">= {stream.level_filter.name}" if stream.level_filter else "all"
        print(f"  {stream.name}: {status}, filter='{stream.filter_query[:20]}', level={level_str}")
        
    # Recent logs
    print("\nüìù Recent Logs:")
    
    recent_query = LogQuery(query_id="recent", limit=5)
    recent_logs = manager.search(recent_query)
    
    for entry in recent_logs:
        level_icon = {
            LogLevel.DEBUG: "üîç",
            LogLevel.INFO: "‚ÑπÔ∏è",
            LogLevel.WARNING: "‚ö†Ô∏è",
            LogLevel.ERROR: "‚ùå",
            LogLevel.CRITICAL: "üö®"
        }.get(entry.level, "‚ùì")
        
        time_str = entry.timestamp.strftime("%H:%M:%S")
        msg = entry.message[:45] + "..." if len(entry.message) > 45 else entry.message
        print(f"  {level_icon} {time_str} [{entry.application[:12]}] {msg}")
        
    # Statistics
    print("\nüìä Platform Statistics:")
    
    print(f"\n  Total Logs: {stats['total_logs']}")
    print(f"  Sources: {stats['sources']}")
    print(f"  Indexes: {stats['indexes']}")
    print(f"  Streams: {stats['streams']}")
    print(f"  Alerts: {stats['alerts']}")
    print(f"  Retention Policies: {stats['policies']}")
    
    # Dashboard
    print("\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ                    Log Aggregation Dashboard                        ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print(f"‚îÇ Total Logs:                    {stats['total_logs']:>12}                        ‚îÇ")
    print(f"‚îÇ Log Sources:                   {stats['sources']:>12}                        ‚îÇ")
    print(f"‚îÇ Indexes:                       {stats['indexes']:>12}                        ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print(f"‚îÇ Active Streams:                {stats['streams']:>12}                        ‚îÇ")
    print(f"‚îÇ Alert Rules:                   {stats['alerts']:>12}                        ‚îÇ")
    print(f"‚îÇ Retention Policies:            {stats['policies']:>12}                        ‚îÇ")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    
    print("\n" + "=" * 60)
    print("Log Aggregation Platform initialized!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
