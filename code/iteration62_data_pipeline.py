#!/usr/bin/env python3
"""
Server Init - Iteration 62: Data Pipeline & ETL Platform
–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ ETL

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- Pipeline Orchestration - –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
- Data Extraction - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- Data Transformation - —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- Data Loading - –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- Scheduling - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
- Data Quality - –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- Lineage Tracking - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è
- Incremental Processing - –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
"""

import json
import asyncio
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
from collections import defaultdict
import uuid
import hashlib


class PipelineStatus(Enum):
    """–°—Ç–∞—Ç—É—Å –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    IDLE = "idle"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class TaskStatus(Enum):
    """–°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"
    UPSTREAM_FAILED = "upstream_failed"


class DataSourceType(Enum):
    """–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    DATABASE = "database"
    API = "api"
    FILE = "file"
    STREAM = "stream"
    S3 = "s3"
    KAFKA = "kafka"


class TransformType(Enum):
    """–¢–∏–ø —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    MAP = "map"
    FILTER = "filter"
    AGGREGATE = "aggregate"
    JOIN = "join"
    PIVOT = "pivot"
    CUSTOM = "custom"


class ScheduleType(Enum):
    """–¢–∏–ø —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è"""
    CRON = "cron"
    INTERVAL = "interval"
    MANUAL = "manual"
    EVENT = "event"


@dataclass
class DataSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö"""
    source_id: str
    name: str
    source_type: DataSourceType
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    connection_config: Dict[str, Any] = field(default_factory=dict)
    
    # –°—Ö–µ–º–∞
    schema: Dict[str, str] = field(default_factory=dict)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    description: str = ""
    tags: List[str] = field(default_factory=list)


@dataclass
class DataDestination:
    """–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
    destination_id: str
    name: str
    destination_type: DataSourceType
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    connection_config: Dict[str, Any] = field(default_factory=dict)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–ø–∏—Å–∏
    write_mode: str = "append"  # append, overwrite, merge
    partition_by: List[str] = field(default_factory=list)


@dataclass
class Transform:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è"""
    transform_id: str
    name: str
    transform_type: TransformType
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config: Dict[str, Any] = field(default_factory=dict)
    
    # SQL (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
    sql: str = ""
    
    # –§—É–Ω–∫—Ü–∏—è (–¥–ª—è custom)
    function_name: str = ""


@dataclass
class Task:
    """–ó–∞–¥–∞—á–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ"""
    task_id: str
    name: str
    
    # –¢–∏–ø –∑–∞–¥–∞—á–∏
    task_type: str = "transform"  # extract, transform, load, custom
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config: Dict[str, Any] = field(default_factory=dict)
    
    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    depends_on: List[str] = field(default_factory=list)
    
    # –°—Ç–∞—Ç—É—Å
    status: TaskStatus = TaskStatus.PENDING
    
    # –í—Ä–µ–º—è
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output: Any = None
    error: Optional[str] = None
    
    # Retry
    retries: int = 0
    max_retries: int = 3


@dataclass
class Pipeline:
    """–ü–∞–π–ø–ª–∞–π–Ω –¥–∞–Ω–Ω—ã—Ö"""
    pipeline_id: str
    name: str
    
    # –û–ø–∏—Å–∞–Ω–∏–µ
    description: str = ""
    
    # –ó–∞–¥–∞—á–∏
    tasks: List[Task] = field(default_factory=list)
    
    # –°—Ç–∞—Ç—É—Å
    status: PipelineStatus = PipelineStatus.IDLE
    
    # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ
    schedule_type: ScheduleType = ScheduleType.MANUAL
    schedule_config: Dict[str, Any] = field(default_factory=dict)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    parameters: Dict[str, Any] = field(default_factory=dict)
    
    # –í—Ä–µ–º—è
    created_at: datetime = field(default_factory=datetime.now)
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_runs: int = 0
    successful_runs: int = 0
    failed_runs: int = 0


@dataclass
class PipelineRun:
    """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    run_id: str
    pipeline_id: str
    
    # –°—Ç–∞—Ç—É—Å
    status: PipelineStatus = PipelineStatus.RUNNING
    
    # –ó–∞–¥–∞—á–∏
    task_runs: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    parameters: Dict[str, Any] = field(default_factory=dict)
    
    # –í—Ä–µ–º—è
    started_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    records_processed: int = 0
    records_failed: int = 0


@dataclass
class DataQualityRule:
    """–ü—Ä–∞–≤–∏–ª–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    rule_id: str
    name: str
    
    # –¢–∏–ø –ø—Ä–æ–≤–µ—Ä–∫–∏
    check_type: str = ""  # not_null, unique, range, regex, custom
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    column: str = ""
    config: Dict[str, Any] = field(default_factory=dict)
    
    # Severity
    severity: str = "warning"  # warning, error
    
    # –°—Ç–∞—Ç—É—Å
    enabled: bool = True


@dataclass
class DataQualityResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    rule_id: str
    passed: bool
    
    # –î–µ—Ç–∞–ª–∏
    total_records: int = 0
    failed_records: int = 0
    failure_rate: float = 0.0
    
    # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
    sample_failures: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class LineageNode:
    """–£–∑–µ–ª lineage"""
    node_id: str
    node_type: str  # source, transform, destination
    name: str
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LineageEdge:
    """–†–µ–±—Ä–æ lineage"""
    source_id: str
    target_id: str
    
    # –¢–∏–ø —Å–≤—è–∑–∏
    relationship: str = "derived_from"
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata: Dict[str, Any] = field(default_factory=dict)


class DataExtractor:
    """–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.sources: Dict[str, DataSource] = {}
        
    def register_source(self, name: str, source_type: DataSourceType,
                         connection_config: Dict[str, Any], **kwargs) -> DataSource:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        source = DataSource(
            source_id=f"src_{uuid.uuid4().hex[:8]}",
            name=name,
            source_type=source_type,
            connection_config=connection_config,
            **kwargs
        )
        
        self.sources[name] = source
        return source
        
    async def extract(self, source_name: str, query: Dict[str, Any] = None) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        source = self.sources.get(source_name)
        
        if not source:
            raise ValueError(f"Source {source_name} not found")
            
        # –°–∏–º—É–ª—è—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        await asyncio.sleep(0.1)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        data = []
        for i in range(100):
            record = {
                "id": i + 1,
                "name": f"Record_{i + 1}",
                "value": (i + 1) * 10.5,
                "timestamp": datetime.now().isoformat(),
                "source": source_name
            }
            data.append(record)
            
        return data


class DataTransformer:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.transforms: Dict[str, Transform] = {}
        
    def register_transform(self, name: str, transform_type: TransformType,
                            config: Dict[str, Any] = None, **kwargs) -> Transform:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        transform = Transform(
            transform_id=f"tfm_{uuid.uuid4().hex[:8]}",
            name=name,
            transform_type=transform_type,
            config=config or {},
            **kwargs
        )
        
        self.transforms[name] = transform
        return transform
        
    async def transform(self, data: List[Dict], transform_name: str) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        transform = self.transforms.get(transform_name)
        
        if not transform:
            raise ValueError(f"Transform {transform_name} not found")
            
        await asyncio.sleep(0.05)
        
        if transform.transform_type == TransformType.MAP:
            return self._apply_map(data, transform.config)
        elif transform.transform_type == TransformType.FILTER:
            return self._apply_filter(data, transform.config)
        elif transform.transform_type == TransformType.AGGREGATE:
            return self._apply_aggregate(data, transform.config)
            
        return data
        
    def _apply_map(self, data: List[Dict], config: Dict) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ map"""
        mappings = config.get("mappings", {})
        
        result = []
        for record in data:
            new_record = {}
            for new_key, old_key in mappings.items():
                if old_key in record:
                    new_record[new_key] = record[old_key]
            result.append(new_record)
            
        return result
        
    def _apply_filter(self, data: List[Dict], config: Dict) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ filter"""
        column = config.get("column", "")
        operator = config.get("operator", "eq")
        value = config.get("value")
        
        result = []
        for record in data:
            record_value = record.get(column)
            
            if operator == "eq" and record_value == value:
                result.append(record)
            elif operator == "gt" and record_value > value:
                result.append(record)
            elif operator == "lt" and record_value < value:
                result.append(record)
            elif operator == "contains" and value in str(record_value):
                result.append(record)
                
        return result
        
    def _apply_aggregate(self, data: List[Dict], config: Dict) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ aggregate"""
        group_by = config.get("group_by", [])
        aggregations = config.get("aggregations", {})
        
        if not group_by:
            # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
            result = {}
            for agg_name, agg_config in aggregations.items():
                column = agg_config.get("column")
                func = agg_config.get("function", "sum")
                values = [r.get(column, 0) for r in data]
                
                if func == "sum":
                    result[agg_name] = sum(values)
                elif func == "avg":
                    result[agg_name] = sum(values) / len(values) if values else 0
                elif func == "count":
                    result[agg_name] = len(values)
                elif func == "min":
                    result[agg_name] = min(values) if values else 0
                elif func == "max":
                    result[agg_name] = max(values) if values else 0
                    
            return [result]
            
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
        groups = defaultdict(list)
        for record in data:
            key = tuple(record.get(col) for col in group_by)
            groups[key].append(record)
            
        result = []
        for key, group_data in groups.items():
            row = dict(zip(group_by, key))
            
            for agg_name, agg_config in aggregations.items():
                column = agg_config.get("column")
                func = agg_config.get("function", "sum")
                values = [r.get(column, 0) for r in group_data]
                
                if func == "sum":
                    row[agg_name] = sum(values)
                elif func == "avg":
                    row[agg_name] = sum(values) / len(values) if values else 0
                elif func == "count":
                    row[agg_name] = len(values)
                    
            result.append(row)
            
        return result


class DataLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.destinations: Dict[str, DataDestination] = {}
        self.loaded_data: Dict[str, List[Dict]] = {}  # –î–ª—è –¥–µ–º–æ
        
    def register_destination(self, name: str, destination_type: DataSourceType,
                               connection_config: Dict[str, Any], **kwargs) -> DataDestination:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è"""
        destination = DataDestination(
            destination_id=f"dst_{uuid.uuid4().hex[:8]}",
            name=name,
            destination_type=destination_type,
            connection_config=connection_config,
            **kwargs
        )
        
        self.destinations[name] = destination
        return destination
        
    async def load(self, data: List[Dict], destination_name: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        destination = self.destinations.get(destination_name)
        
        if not destination:
            raise ValueError(f"Destination {destination_name} not found")
            
        await asyncio.sleep(0.05)
        
        # –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
        if destination_name not in self.loaded_data:
            self.loaded_data[destination_name] = []
            
        if destination.write_mode == "overwrite":
            self.loaded_data[destination_name] = data
        else:
            self.loaded_data[destination_name].extend(data)
            
        return {
            "destination": destination_name,
            "records_loaded": len(data),
            "write_mode": destination.write_mode,
            "total_records": len(self.loaded_data[destination_name])
        }


class DataQualityEngine:
    """–î–≤–∏–∂–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.rules: Dict[str, DataQualityRule] = {}
        
    def add_rule(self, name: str, check_type: str, column: str,
                  config: Dict[str, Any] = None, **kwargs) -> DataQualityRule:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        rule = DataQualityRule(
            rule_id=f"dq_{uuid.uuid4().hex[:8]}",
            name=name,
            check_type=check_type,
            column=column,
            config=config or {},
            **kwargs
        )
        
        self.rules[name] = rule
        return rule
        
    def validate(self, data: List[Dict], rule_names: List[str] = None) -> List[DataQualityResult]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        rules_to_check = []
        
        if rule_names:
            rules_to_check = [self.rules[n] for n in rule_names if n in self.rules]
        else:
            rules_to_check = [r for r in self.rules.values() if r.enabled]
            
        results = []
        
        for rule in rules_to_check:
            result = self._check_rule(data, rule)
            results.append(result)
            
        return results
        
    def _check_rule(self, data: List[Dict], rule: DataQualityRule) -> DataQualityResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞"""
        failed_records = []
        
        for i, record in enumerate(data):
            value = record.get(rule.column)
            
            if not self._check_value(value, rule):
                failed_records.append({
                    "index": i,
                    "value": value,
                    "rule": rule.name
                })
                
        total = len(data)
        failed = len(failed_records)
        
        return DataQualityResult(
            rule_id=rule.rule_id,
            passed=failed == 0,
            total_records=total,
            failed_records=failed,
            failure_rate=failed / total if total > 0 else 0,
            sample_failures=failed_records[:5]
        )
        
    def _check_value(self, value: Any, rule: DataQualityRule) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è"""
        check_type = rule.check_type
        config = rule.config
        
        if check_type == "not_null":
            return value is not None
            
        elif check_type == "unique":
            # –£–ø—Ä–æ—â—ë–Ω–Ω–æ - –¥–ª—è –ø–æ–ª–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            return True
            
        elif check_type == "range":
            min_val = config.get("min")
            max_val = config.get("max")
            
            if min_val is not None and value < min_val:
                return False
            if max_val is not None and value > max_val:
                return False
            return True
            
        elif check_type == "in_set":
            valid_values = config.get("values", [])
            return value in valid_values
            
        elif check_type == "regex":
            import re
            pattern = config.get("pattern", "")
            return bool(re.match(pattern, str(value))) if value else False
            
        return True


class LineageTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.nodes: Dict[str, LineageNode] = {}
        self.edges: List[LineageEdge] = []
        
    def add_node(self, node_id: str, node_type: str, name: str,
                  metadata: Dict[str, Any] = None) -> LineageNode:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–∞"""
        node = LineageNode(
            node_id=node_id,
            node_type=node_type,
            name=name,
            metadata=metadata or {}
        )
        
        self.nodes[node_id] = node
        return node
        
    def add_edge(self, source_id: str, target_id: str,
                  relationship: str = "derived_from",
                  metadata: Dict[str, Any] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–∏"""
        edge = LineageEdge(
            source_id=source_id,
            target_id=target_id,
            relationship=relationship,
            metadata=metadata or {}
        )
        
        self.edges.append(edge)
        
    def get_upstream(self, node_id: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ upstream —É–∑–ª–æ–≤"""
        upstream = []
        
        for edge in self.edges:
            if edge.target_id == node_id:
                upstream.append(edge.source_id)
                upstream.extend(self.get_upstream(edge.source_id))
                
        return list(set(upstream))
        
    def get_downstream(self, node_id: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ downstream —É–∑–ª–æ–≤"""
        downstream = []
        
        for edge in self.edges:
            if edge.source_id == node_id:
                downstream.append(edge.target_id)
                downstream.extend(self.get_downstream(edge.target_id))
                
        return list(set(downstream))
        
    def get_lineage_graph(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ lineage"""
        return {
            "nodes": [
                {"id": n.node_id, "type": n.node_type, "name": n.name}
                for n in self.nodes.values()
            ],
            "edges": [
                {"source": e.source_id, "target": e.target_id, "relationship": e.relationship}
                for e in self.edges
            ]
        }


class PipelineOrchestrator:
    """–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–æ–≤"""
    
    def __init__(self):
        self.pipelines: Dict[str, Pipeline] = {}
        self.runs: Dict[str, PipelineRun] = {}
        
        self.extractor = DataExtractor()
        self.transformer = DataTransformer()
        self.loader = DataLoader()
        self.quality_engine = DataQualityEngine()
        self.lineage_tracker = LineageTracker()
        
    def create_pipeline(self, name: str, tasks: List[Dict[str, Any]],
                         **kwargs) -> Pipeline:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        task_objects = []
        
        for task_config in tasks:
            task = Task(
                task_id=f"task_{uuid.uuid4().hex[:8]}",
                name=task_config.get("name", ""),
                task_type=task_config.get("type", "transform"),
                config=task_config.get("config", {}),
                depends_on=task_config.get("depends_on", [])
            )
            task_objects.append(task)
            
        pipeline = Pipeline(
            pipeline_id=f"pipe_{uuid.uuid4().hex[:8]}",
            name=name,
            tasks=task_objects,
            **kwargs
        )
        
        self.pipelines[name] = pipeline
        return pipeline
        
    async def run_pipeline(self, pipeline_name: str,
                            parameters: Dict[str, Any] = None) -> PipelineRun:
        """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        pipeline = self.pipelines.get(pipeline_name)
        
        if not pipeline:
            raise ValueError(f"Pipeline {pipeline_name} not found")
            
        run = PipelineRun(
            run_id=f"run_{uuid.uuid4().hex[:8]}",
            pipeline_id=pipeline.pipeline_id,
            parameters=parameters or {}
        )
        
        pipeline.status = PipelineStatus.RUNNING
        self.runs[run.run_id] = run
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        task_results: Dict[str, Any] = {}
        
        try:
            for task in self._get_execution_order(pipeline.tasks):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                deps_ok = all(
                    run.task_runs.get(dep, {}).get("status") == TaskStatus.SUCCESS.value
                    for dep in task.depends_on
                )
                
                if not deps_ok:
                    task.status = TaskStatus.UPSTREAM_FAILED
                    run.task_runs[task.task_id] = {
                        "status": TaskStatus.UPSTREAM_FAILED.value,
                        "error": "Upstream task failed"
                    }
                    continue
                    
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É
                task.status = TaskStatus.RUNNING
                task.started_at = datetime.now()
                
                try:
                    result = await self._execute_task(task, task_results, parameters or {})
                    task_results[task.task_id] = result
                    
                    task.status = TaskStatus.SUCCESS
                    task.output = result
                    run.records_processed += len(result) if isinstance(result, list) else 1
                    
                    run.task_runs[task.task_id] = {
                        "status": TaskStatus.SUCCESS.value,
                        "records": len(result) if isinstance(result, list) else 1
                    }
                    
                except Exception as e:
                    task.status = TaskStatus.FAILED
                    task.error = str(e)
                    
                    run.task_runs[task.task_id] = {
                        "status": TaskStatus.FAILED.value,
                        "error": str(e)
                    }
                    
                task.completed_at = datetime.now()
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
            failed_tasks = [t for t in pipeline.tasks if t.status == TaskStatus.FAILED]
            
            if failed_tasks:
                run.status = PipelineStatus.FAILED
                pipeline.status = PipelineStatus.FAILED
                pipeline.failed_runs += 1
            else:
                run.status = PipelineStatus.SUCCESS
                pipeline.status = PipelineStatus.SUCCESS
                pipeline.successful_runs += 1
                
        except Exception as e:
            run.status = PipelineStatus.FAILED
            pipeline.status = PipelineStatus.FAILED
            pipeline.failed_runs += 1
            
        run.completed_at = datetime.now()
        pipeline.total_runs += 1
        pipeline.last_run = datetime.now()
        
        return run
        
    def _get_execution_order(self, tasks: List[Task]) -> List[Task]:
        """–¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á"""
        task_map = {t.task_id: t for t in tasks}
        task_name_map = {t.name: t.task_id for t in tasks}
        
        visited = set()
        order = []
        
        def visit(task_id: str):
            if task_id in visited:
                return
            visited.add(task_id)
            
            task = task_map.get(task_id)
            if task:
                for dep_name in task.depends_on:
                    dep_id = task_name_map.get(dep_name)
                    if dep_id:
                        visit(dep_id)
                order.append(task)
                
        for task in tasks:
            visit(task.task_id)
            
        return order
        
    async def _execute_task(self, task: Task, previous_results: Dict[str, Any],
                             parameters: Dict[str, Any]) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏"""
        task_type = task.task_type
        config = task.config
        
        if task_type == "extract":
            source = config.get("source")
            return await self.extractor.extract(source, config.get("query"))
            
        elif task_type == "transform":
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
            input_task = config.get("input")
            input_task_id = None
            
            for tid, t in previous_results.items():
                # –ò—â–µ–º –ø–æ –∏–º–µ–Ω–∏ –∑–∞–¥–∞—á–∏
                pass
                
            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω input
            data = list(previous_results.values())[-1] if previous_results else []
            
            transform_name = config.get("transform")
            return await self.transformer.transform(data, transform_name)
            
        elif task_type == "load":
            data = list(previous_results.values())[-1] if previous_results else []
            destination = config.get("destination")
            result = await self.loader.load(data, destination)
            return [result]
            
        elif task_type == "quality_check":
            data = list(previous_results.values())[-1] if previous_results else []
            rules = config.get("rules", [])
            return self.quality_engine.validate(data, rules)
            
        return []
        
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        return {
            "pipelines": len(self.pipelines),
            "total_runs": sum(p.total_runs for p in self.pipelines.values()),
            "successful_runs": sum(p.successful_runs for p in self.pipelines.values()),
            "failed_runs": sum(p.failed_runs for p in self.pipelines.values()),
            "sources": len(self.extractor.sources),
            "destinations": len(self.loader.destinations),
            "transforms": len(self.transformer.transforms),
            "quality_rules": len(self.quality_engine.rules)
        }


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
if __name__ == "__main__":
    print("=" * 60)
    print("Server Init - Iteration 62: Data Pipeline & ETL Platform")
    print("=" * 60)
    
    async def demo():
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
        orchestrator = PipelineOrchestrator()
        print("‚úì Pipeline Orchestrator created")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print("\nüì• Registering data sources...")
        
        orchestrator.extractor.register_source(
            name="sales_db",
            source_type=DataSourceType.DATABASE,
            connection_config={
                "host": "localhost",
                "port": 5432,
                "database": "sales"
            },
            schema={
                "id": "integer",
                "product": "string",
                "amount": "decimal",
                "date": "timestamp"
            }
        )
        print("  ‚úì Source: sales_db")
        
        orchestrator.extractor.register_source(
            name="user_api",
            source_type=DataSourceType.API,
            connection_config={
                "url": "https://api.example.com/users",
                "auth": "bearer"
            }
        )
        print("  ‚úì Source: user_api")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        print("\nüîÑ Registering transforms...")
        
        orchestrator.transformer.register_transform(
            name="map_sales",
            transform_type=TransformType.MAP,
            config={
                "mappings": {
                    "record_id": "id",
                    "record_name": "name",
                    "amount": "value"
                }
            }
        )
        print("  ‚úì Transform: map_sales")
        
        orchestrator.transformer.register_transform(
            name="filter_high_value",
            transform_type=TransformType.FILTER,
            config={
                "column": "value",
                "operator": "gt",
                "value": 500
            }
        )
        print("  ‚úì Transform: filter_high_value")
        
        orchestrator.transformer.register_transform(
            name="aggregate_by_source",
            transform_type=TransformType.AGGREGATE,
            config={
                "group_by": ["source"],
                "aggregations": {
                    "total_amount": {"column": "amount", "function": "sum"},
                    "record_count": {"column": "id", "function": "count"}
                }
            }
        )
        print("  ‚úì Transform: aggregate_by_source")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π
        print("\nüì§ Registering destinations...")
        
        orchestrator.loader.register_destination(
            name="data_warehouse",
            destination_type=DataSourceType.DATABASE,
            connection_config={
                "host": "dw.example.com",
                "port": 5432,
                "database": "warehouse"
            },
            write_mode="append"
        )
        print("  ‚úì Destination: data_warehouse")
        
        # –ü—Ä–∞–≤–∏–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n‚úÖ Adding data quality rules...")
        
        orchestrator.quality_engine.add_rule(
            name="id_not_null",
            check_type="not_null",
            column="id",
            severity="error"
        )
        print("  ‚úì Rule: id_not_null")
        
        orchestrator.quality_engine.add_rule(
            name="value_in_range",
            check_type="range",
            column="value",
            config={"min": 0, "max": 10000},
            severity="warning"
        )
        print("  ‚úì Rule: value_in_range")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("\nüìä Creating pipeline...")
        
        pipeline = orchestrator.create_pipeline(
            name="sales_etl",
            description="Sales data ETL pipeline",
            tasks=[
                {
                    "name": "extract_sales",
                    "type": "extract",
                    "config": {"source": "sales_db"}
                },
                {
                    "name": "map_fields",
                    "type": "transform",
                    "config": {"transform": "map_sales"},
                    "depends_on": ["extract_sales"]
                },
                {
                    "name": "filter_data",
                    "type": "transform",
                    "config": {"transform": "filter_high_value"},
                    "depends_on": ["map_fields"]
                },
                {
                    "name": "quality_check",
                    "type": "quality_check",
                    "config": {"rules": ["id_not_null", "value_in_range"]},
                    "depends_on": ["filter_data"]
                },
                {
                    "name": "load_warehouse",
                    "type": "load",
                    "config": {"destination": "data_warehouse"},
                    "depends_on": ["quality_check"]
                }
            ]
        )
        print(f"  ‚úì Pipeline: {pipeline.name}")
        print(f"  Tasks: {len(pipeline.tasks)}")
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("\nüöÄ Running pipeline...")
        
        run = await orchestrator.run_pipeline("sales_etl")
        
        print(f"  Run ID: {run.run_id}")
        print(f"  Status: {run.status.value}")
        print(f"  Records processed: {run.records_processed}")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–¥–∞—á
        print("\n  Task results:")
        for task_id, result in run.task_runs.items():
            status = result.get("status")
            records = result.get("records", "-")
            print(f"    {task_id[:12]}...: {status} ({records} records)")
            
        # Lineage tracking
        print("\nüîó Data Lineage...")
        
        orchestrator.lineage_tracker.add_node("sales_db", "source", "Sales Database")
        orchestrator.lineage_tracker.add_node("map_sales", "transform", "Field Mapping")
        orchestrator.lineage_tracker.add_node("filter_high", "transform", "High Value Filter")
        orchestrator.lineage_tracker.add_node("data_warehouse", "destination", "Data Warehouse")
        
        orchestrator.lineage_tracker.add_edge("sales_db", "map_sales")
        orchestrator.lineage_tracker.add_edge("map_sales", "filter_high")
        orchestrator.lineage_tracker.add_edge("filter_high", "data_warehouse")
        
        lineage = orchestrator.lineage_tracker.get_lineage_graph()
        print(f"  Nodes: {len(lineage['nodes'])}")
        print(f"  Edges: {len(lineage['edges'])}")
        
        # Upstream/downstream
        upstream = orchestrator.lineage_tracker.get_upstream("data_warehouse")
        print(f"  Upstream of data_warehouse: {upstream}")
        
        # –í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫
        print("\nüîÑ Running pipeline again...")
        
        run2 = await orchestrator.run_pipeline("sales_etl")
        print(f"  Run 2 Status: {run2.status.value}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\nüìà Platform Statistics:")
        stats = orchestrator.get_stats()
        print(f"  Pipelines: {stats['pipelines']}")
        print(f"  Total runs: {stats['total_runs']}")
        print(f"  Successful: {stats['successful_runs']}")
        print(f"  Failed: {stats['failed_runs']}")
        print(f"  Sources: {stats['sources']}")
        print(f"  Destinations: {stats['destinations']}")
        print(f"  Transforms: {stats['transforms']}")
        print(f"  Quality rules: {stats['quality_rules']}")
        
    asyncio.run(demo())
    
    print("\n" + "=" * 60)
    print("Data Pipeline & ETL Platform initialized!")
    print("=" * 60)
