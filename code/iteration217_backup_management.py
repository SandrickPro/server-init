#!/usr/bin/env python3
"""
Server Init - Iteration 217: Backup Management Platform
ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°Ð¼Ð¸

Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»:
- Backup Scheduling - Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð¾Ð²
- Retention Policies - Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
- Incremental Backups - Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð±ÑÐºÐ°Ð¿Ñ‹
- Restore Testing - Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
- Cross-Region Replication - ÐºÑ€Ð¾ÑÑ-Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ
- Encryption - ÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
- Verification - Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ
- Disaster Recovery - Ð°Ð²Ð°Ñ€Ð¸Ð¹Ð½Ð¾Ðµ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ
"""

import asyncio
import random
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from enum import Enum
import uuid


class BackupType(Enum):
    """Ð¢Ð¸Ð¿ Ð±ÑÐºÐ°Ð¿Ð°"""
    FULL = "full"
    INCREMENTAL = "incremental"
    DIFFERENTIAL = "differential"
    SNAPSHOT = "snapshot"


class BackupStatus(Enum):
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð±ÑÐºÐ°Ð¿Ð°"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    VERIFYING = "verifying"
    VERIFIED = "verified"


class StorageType(Enum):
    """Ð¢Ð¸Ð¿ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°"""
    LOCAL = "local"
    S3 = "s3"
    GCS = "gcs"
    AZURE_BLOB = "azure_blob"
    NFS = "nfs"


class ScheduleFrequency(Enum):
    """Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð° Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ñ"""
    HOURLY = "hourly"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"


class EncryptionType(Enum):
    """Ð¢Ð¸Ð¿ ÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ"""
    NONE = "none"
    AES_256 = "aes-256"
    AES_128 = "aes-128"


class RestoreStatus(Enum):
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class BackupTarget:
    """Ð¦ÐµÐ»ÑŒ Ð±ÑÐºÐ°Ð¿Ð°"""
    target_id: str
    name: str = ""
    target_type: str = ""  # database, filesystem, volume, etc.
    
    # Connection
    connection_string: str = ""
    
    # Size
    estimated_size_gb: float = 0
    
    # Include/Exclude
    include_paths: List[str] = field(default_factory=list)
    exclude_paths: List[str] = field(default_factory=list)


@dataclass
class BackupStorage:
    """Ð¥Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð±ÑÐºÐ°Ð¿Ð¾Ð²"""
    storage_id: str
    name: str = ""
    storage_type: StorageType = StorageType.S3
    
    # Location
    bucket: str = ""
    path: str = ""
    region: str = ""
    
    # Credentials
    credentials_secret: str = ""
    
    # Encryption
    encryption: EncryptionType = EncryptionType.AES_256


@dataclass
class RetentionPolicy:
    """ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸ÐºÐ° Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ"""
    policy_id: str
    name: str = ""
    
    # Retention periods
    hourly_retention: int = 24  # Keep 24 hourly backups
    daily_retention: int = 7   # Keep 7 daily backups
    weekly_retention: int = 4  # Keep 4 weekly backups
    monthly_retention: int = 12  # Keep 12 monthly backups
    
    # Minimum
    min_backups: int = 3
    
    # Maximum age
    max_age_days: int = 365


@dataclass
class BackupSchedule:
    """Ð Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð°"""
    schedule_id: str
    name: str = ""
    
    # Target
    target_id: str = ""
    
    # Schedule
    frequency: ScheduleFrequency = ScheduleFrequency.DAILY
    time: str = "02:00"  # 24h format
    day_of_week: int = 0  # 0=Monday for weekly
    day_of_month: int = 1  # for monthly
    
    # Type
    backup_type: BackupType = BackupType.INCREMENTAL
    full_backup_frequency: int = 7  # Full backup every N incremental
    
    # Storage
    storage_id: str = ""
    
    # Retention
    retention_policy_id: str = ""
    
    # Active
    active: bool = True
    
    # Last run
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None


@dataclass
class Backup:
    """Ð‘ÑÐºÐ°Ð¿"""
    backup_id: str
    name: str = ""
    
    # Target
    target_id: str = ""
    target_name: str = ""
    
    # Type
    backup_type: BackupType = BackupType.FULL
    
    # Status
    status: BackupStatus = BackupStatus.PENDING
    
    # Time
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # Size
    size_bytes: int = 0
    compressed_size_bytes: int = 0
    
    # Storage
    storage_id: str = ""
    storage_path: str = ""
    
    # Verification
    checksum: str = ""
    verified: bool = False
    
    # Parent (for incremental)
    parent_backup_id: Optional[str] = None
    
    # Encryption
    encrypted: bool = True
    
    # Metadata
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RestoreJob:
    """Ð—Ð°Ð´Ð°Ñ‡Ð° Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ"""
    job_id: str
    backup_id: str = ""
    
    # Target
    restore_target: str = ""  # Where to restore
    
    # Status
    status: RestoreStatus = RestoreStatus.PENDING
    
    # Progress
    progress_percent: float = 0
    
    # Time
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # Options
    point_in_time: Optional[datetime] = None
    restore_to_new: bool = True  # Restore to new location vs overwrite


@dataclass
class ReplicationConfig:
    """ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸"""
    config_id: str
    name: str = ""
    
    # Source and destination
    source_storage_id: str = ""
    destination_storage_id: str = ""
    
    # Options
    enabled: bool = True
    async_replication: bool = True
    
    # Stats
    last_replicated: Optional[datetime] = None
    replication_lag_minutes: int = 0


class BackupExecutor:
    """Ð˜ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒ Ð±ÑÐºÐ°Ð¿Ð¾Ð²"""
    
    async def execute(self, target: BackupTarget, backup: Backup,
                     parent_backup: Optional[Backup] = None) -> bool:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð°"""
        backup.status = BackupStatus.RUNNING
        backup.started_at = datetime.now()
        
        # Simulate backup execution
        await asyncio.sleep(random.uniform(0.1, 0.3))
        
        # Calculate size
        base_size = int(target.estimated_size_gb * 1024 * 1024 * 1024)
        
        if backup.backup_type == BackupType.FULL:
            backup.size_bytes = base_size
        elif backup.backup_type == BackupType.INCREMENTAL:
            # Incremental is typically 5-20% of full
            backup.size_bytes = int(base_size * random.uniform(0.05, 0.2))
        elif backup.backup_type == BackupType.DIFFERENTIAL:
            # Differential grows over time
            backup.size_bytes = int(base_size * random.uniform(0.1, 0.4))
        else:  # Snapshot
            backup.size_bytes = int(base_size * random.uniform(0.01, 0.1))
            
        # Compression (typically 40-60% compression)
        compression_ratio = random.uniform(0.4, 0.6)
        backup.compressed_size_bytes = int(backup.size_bytes * compression_ratio)
        
        # Generate checksum
        backup.checksum = hashlib.sha256(
            f"{backup.backup_id}{backup.size_bytes}".encode()
        ).hexdigest()[:16]
        
        backup.status = BackupStatus.COMPLETED
        backup.completed_at = datetime.now()
        
        return True


class BackupVerifier:
    """Ð’ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð±ÑÐºÐ°Ð¿Ð¾Ð²"""
    
    async def verify(self, backup: Backup) -> bool:
        """Ð’ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°"""
        backup.status = BackupStatus.VERIFYING
        
        # Simulate verification
        await asyncio.sleep(random.uniform(0.05, 0.15))
        
        # 99% success rate
        success = random.random() > 0.01
        
        backup.verified = success
        backup.status = BackupStatus.VERIFIED if success else BackupStatus.FAILED
        
        return success


class RestoreExecutor:
    """Ð˜ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ"""
    
    async def execute(self, job: RestoreJob, backup: Backup) -> bool:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ"""
        job.status = RestoreStatus.RUNNING
        job.started_at = datetime.now()
        
        # Simulate restore progress
        for progress in range(0, 101, 10):
            job.progress_percent = progress
            await asyncio.sleep(0.02)
            
        job.status = RestoreStatus.COMPLETED
        job.completed_at = datetime.now()
        
        return True


class BackupManagementPlatform:
    """ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°Ð¼Ð¸"""
    
    def __init__(self):
        self.targets: Dict[str, BackupTarget] = {}
        self.storages: Dict[str, BackupStorage] = {}
        self.schedules: Dict[str, BackupSchedule] = {}
        self.backups: Dict[str, Backup] = {}
        self.retention_policies: Dict[str, RetentionPolicy] = {}
        self.restore_jobs: Dict[str, RestoreJob] = {}
        self.replications: Dict[str, ReplicationConfig] = {}
        
        self.executor = BackupExecutor()
        self.verifier = BackupVerifier()
        self.restore_executor = RestoreExecutor()
        
    def register_target(self, name: str, target_type: str,
                       connection_string: str = "",
                       estimated_size_gb: float = 10) -> BackupTarget:
        """Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ñ†ÐµÐ»Ð¸"""
        target = BackupTarget(
            target_id=f"target_{uuid.uuid4().hex[:8]}",
            name=name,
            target_type=target_type,
            connection_string=connection_string,
            estimated_size_gb=estimated_size_gb
        )
        self.targets[target.target_id] = target
        return target
        
    def add_storage(self, name: str, storage_type: StorageType,
                   bucket: str = "", region: str = "us-east-1") -> BackupStorage:
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°"""
        storage = BackupStorage(
            storage_id=f"storage_{uuid.uuid4().hex[:8]}",
            name=name,
            storage_type=storage_type,
            bucket=bucket,
            region=region
        )
        self.storages[storage.storage_id] = storage
        return storage
        
    def create_retention_policy(self, name: str,
                               daily: int = 7, weekly: int = 4,
                               monthly: int = 12) -> RetentionPolicy:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ"""
        policy = RetentionPolicy(
            policy_id=f"retention_{uuid.uuid4().hex[:8]}",
            name=name,
            daily_retention=daily,
            weekly_retention=weekly,
            monthly_retention=monthly
        )
        self.retention_policies[policy.policy_id] = policy
        return policy
        
    def create_schedule(self, name: str, target_id: str,
                       storage_id: str, frequency: ScheduleFrequency,
                       retention_policy_id: str = "",
                       backup_type: BackupType = BackupType.INCREMENTAL) -> BackupSchedule:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ñ"""
        schedule = BackupSchedule(
            schedule_id=f"schedule_{uuid.uuid4().hex[:8]}",
            name=name,
            target_id=target_id,
            storage_id=storage_id,
            frequency=frequency,
            backup_type=backup_type,
            retention_policy_id=retention_policy_id,
            next_run=datetime.now() + timedelta(hours=1)
        )
        self.schedules[schedule.schedule_id] = schedule
        return schedule
        
    async def run_backup(self, target_id: str, backup_type: BackupType,
                        storage_id: str) -> Optional[Backup]:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð°"""
        target = self.targets.get(target_id)
        storage = self.storages.get(storage_id)
        
        if not target or not storage:
            return None
            
        # Find parent for incremental
        parent_backup = None
        if backup_type == BackupType.INCREMENTAL:
            for b in sorted(self.backups.values(), key=lambda x: x.started_at or datetime.min, reverse=True):
                if b.target_id == target_id and b.status == BackupStatus.COMPLETED:
                    parent_backup = b
                    break
                    
        backup = Backup(
            backup_id=f"backup_{uuid.uuid4().hex[:8]}",
            name=f"{target.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            target_id=target_id,
            target_name=target.name,
            backup_type=backup_type,
            storage_id=storage_id,
            storage_path=f"backups/{target.name}/{datetime.now().strftime('%Y/%m/%d')}/",
            parent_backup_id=parent_backup.backup_id if parent_backup else None
        )
        
        self.backups[backup.backup_id] = backup
        
        # Execute
        await self.executor.execute(target, backup, parent_backup)
        
        return backup
        
    async def verify_backup(self, backup_id: str) -> bool:
        """Ð’ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°"""
        backup = self.backups.get(backup_id)
        if not backup:
            return False
            
        return await self.verifier.verify(backup)
        
    async def restore_backup(self, backup_id: str,
                            restore_target: str = "") -> Optional[RestoreJob]:
        """Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· Ð±ÑÐºÐ°Ð¿Ð°"""
        backup = self.backups.get(backup_id)
        if not backup:
            return None
            
        job = RestoreJob(
            job_id=f"restore_{uuid.uuid4().hex[:8]}",
            backup_id=backup_id,
            restore_target=restore_target or backup.target_name + "_restored"
        )
        
        self.restore_jobs[job.job_id] = job
        
        await self.restore_executor.execute(job, backup)
        
        return job
        
    def setup_replication(self, name: str, source_id: str,
                         dest_id: str) -> ReplicationConfig:
        """ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸"""
        config = ReplicationConfig(
            config_id=f"repl_{uuid.uuid4().hex[:8]}",
            name=name,
            source_storage_id=source_id,
            destination_storage_id=dest_id
        )
        self.replications[config.config_id] = config
        return config
        
    def apply_retention(self, target_id: str, policy_id: str) -> int:
        """ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ"""
        policy = self.retention_policies.get(policy_id)
        if not policy:
            return 0
            
        target_backups = [b for b in self.backups.values()
                        if b.target_id == target_id and b.status == BackupStatus.COMPLETED]
        
        # Sort by date
        target_backups.sort(key=lambda x: x.completed_at or datetime.min)
        
        # Simulate retention (keep min_backups)
        if len(target_backups) > policy.min_backups:
            # Would delete old ones beyond retention
            to_delete = len(target_backups) - policy.min_backups
            return min(to_delete, 2)  # Simulate deleting some
            
        return 0
        
    def get_statistics(self) -> Dict[str, Any]:
        """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°"""
        completed = [b for b in self.backups.values() if b.status == BackupStatus.COMPLETED]
        total_size = sum(b.size_bytes for b in completed)
        compressed_size = sum(b.compressed_size_bytes for b in completed)
        
        return {
            "total_targets": len(self.targets),
            "total_storages": len(self.storages),
            "total_schedules": len(self.schedules),
            "active_schedules": len([s for s in self.schedules.values() if s.active]),
            "total_backups": len(self.backups),
            "completed_backups": len(completed),
            "verified_backups": len([b for b in completed if b.verified]),
            "failed_backups": len([b for b in self.backups.values() if b.status == BackupStatus.FAILED]),
            "total_size_gb": total_size / (1024**3),
            "compressed_size_gb": compressed_size / (1024**3),
            "compression_ratio": compressed_size / total_size if total_size > 0 else 0,
            "restore_jobs": len(self.restore_jobs),
            "replications": len(self.replications)
        }


# Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ
async def main():
    print("=" * 60)
    print("Server Init - Iteration 217: Backup Management Platform")
    print("=" * 60)
    
    platform = BackupManagementPlatform()
    print("âœ“ Backup Management Platform created")
    
    # Register targets
    print("\nðŸŽ¯ Registering Backup Targets...")
    
    targets_config = [
        ("Production Database", "postgresql", "postgres://prod:5432/main", 500),
        ("User Data", "filesystem", "/data/users", 200),
        ("Application Logs", "elasticsearch", "es://logs:9200", 100),
        ("Configuration Store", "etcd", "etcd://config:2379", 5),
        ("Media Files", "s3", "s3://media-bucket", 1000),
    ]
    
    targets = []
    for name, ttype, conn, size in targets_config:
        target = platform.register_target(name, ttype, conn, size)
        targets.append(target)
        print(f"  âœ“ {name}: {size}GB estimated")
        
    # Add storage locations
    print("\nðŸ’¾ Adding Storage Locations...")
    
    primary_storage = platform.add_storage(
        "Primary S3",
        StorageType.S3,
        "company-backups-primary",
        "us-east-1"
    )
    print(f"  âœ“ {primary_storage.name}: {primary_storage.region}")
    
    secondary_storage = platform.add_storage(
        "Secondary GCS",
        StorageType.GCS,
        "company-backups-dr",
        "us-west-1"
    )
    print(f"  âœ“ {secondary_storage.name}: {secondary_storage.region}")
    
    local_storage = platform.add_storage(
        "Local NFS",
        StorageType.NFS,
        "/mnt/backup",
        "local"
    )
    print(f"  âœ“ {local_storage.name}: local")
    
    # Create retention policies
    print("\nðŸ“‹ Creating Retention Policies...")
    
    standard_policy = platform.create_retention_policy("Standard", 7, 4, 12)
    print(f"  âœ“ {standard_policy.name}: {standard_policy.daily_retention}d, {standard_policy.weekly_retention}w, {standard_policy.monthly_retention}m")
    
    critical_policy = platform.create_retention_policy("Critical", 14, 8, 24)
    print(f"  âœ“ {critical_policy.name}: {critical_policy.daily_retention}d, {critical_policy.weekly_retention}w, {critical_policy.monthly_retention}m")
    
    # Create schedules
    print("\nâ° Creating Backup Schedules...")
    
    for target in targets[:3]:
        schedule = platform.create_schedule(
            f"{target.name} Daily",
            target.target_id,
            primary_storage.storage_id,
            ScheduleFrequency.DAILY,
            standard_policy.policy_id
        )
        print(f"  âœ“ {schedule.name}: {schedule.frequency.value} at {schedule.time}")
        
    # Run backups
    print("\nðŸ”„ Running Backups...")
    
    backups = []
    for target in targets:
        # Full backup first
        backup = await platform.run_backup(
            target.target_id,
            BackupType.FULL,
            primary_storage.storage_id
        )
        if backup:
            backups.append(backup)
            size_gb = backup.compressed_size_bytes / (1024**3)
            print(f"  âœ“ {backup.name}: {size_gb:.2f}GB compressed")
            
    # Run incremental backups
    print("\nðŸ“ˆ Running Incremental Backups...")
    
    for target in targets[:3]:
        backup = await platform.run_backup(
            target.target_id,
            BackupType.INCREMENTAL,
            primary_storage.storage_id
        )
        if backup:
            backups.append(backup)
            size_mb = backup.compressed_size_bytes / (1024**2)
            print(f"  âœ“ {backup.name}: {size_mb:.2f}MB (incremental)")
            
    # Verify backups
    print("\nâœ… Verifying Backups...")
    
    verified_count = 0
    for backup in backups[:5]:
        success = await platform.verify_backup(backup.backup_id)
        status = "âœ“ verified" if success else "âœ— failed"
        print(f"  {status}: {backup.name}")
        if success:
            verified_count += 1
            
    # Setup replication
    print("\nðŸ” Setting Up Cross-Region Replication...")
    
    replication = platform.setup_replication(
        "US-East to US-West DR",
        primary_storage.storage_id,
        secondary_storage.storage_id
    )
    print(f"  âœ“ {replication.name}: async replication enabled")
    
    # Test restore
    print("\nðŸ”§ Testing Restore...")
    
    test_backup = backups[0] if backups else None
    if test_backup:
        job = await platform.restore_backup(
            test_backup.backup_id,
            "test_restore_target"
        )
        if job:
            print(f"  âœ“ Restore job {job.job_id[:12]}: {job.status.value}")
            print(f"    Target: {job.restore_target}")
            
    # Display backups
    print("\nðŸ’¾ Backup Inventory:")
    
    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Backup                     â”‚ Type     â”‚ Size       â”‚ Status   â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for backup in backups[:8]:
        name = backup.name[:26].ljust(26)
        btype = backup.backup_type.value[:8].ljust(8)
        size = f"{backup.compressed_size_bytes / (1024**3):.2f}GB"[:10].ljust(10)
        
        status_icons = {
            BackupStatus.COMPLETED: "ðŸŸ¢",
            BackupStatus.VERIFIED: "âœ…",
            BackupStatus.RUNNING: "ðŸ”µ",
            BackupStatus.FAILED: "ðŸ”´"
        }
        status = f"{status_icons.get(backup.status, 'âšª')} {backup.status.value[:5]}"[:8].ljust(8)
        
        print(f"  â”‚ {name} â”‚ {btype} â”‚ {size} â”‚ {status} â”‚")
        
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Storage usage
    print("\nðŸ“Š Storage Usage:")
    
    storage_usage = {}
    for backup in backups:
        if backup.storage_id not in storage_usage:
            storage_usage[backup.storage_id] = 0
        storage_usage[backup.storage_id] += backup.compressed_size_bytes
        
    for storage in platform.storages.values():
        used = storage_usage.get(storage.storage_id, 0) / (1024**3)
        bar_len = min(20, int(used / 50))  # Scale bar
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        print(f"  {storage.name:15s} [{bar}] {used:.2f}GB")
        
    # Backup timeline
    print("\nðŸ“… Backup Timeline (Today):")
    
    by_hour = {}
    for backup in backups:
        if backup.completed_at:
            hour = backup.completed_at.hour
            if hour not in by_hour:
                by_hour[hour] = []
            by_hour[hour].append(backup)
            
    for hour in sorted(by_hour.keys()):
        backup_list = by_hour[hour]
        count = len(backup_list)
        bar = "â–“" * count
        print(f"  {hour:02d}:00 â”‚ {bar} ({count})")
        
    # Schedules
    print("\nâ° Active Schedules:")
    
    for schedule in platform.schedules.values():
        target = platform.targets.get(schedule.target_id)
        target_name = target.name if target else "unknown"
        
        print(f"  ðŸ“… {schedule.name}:")
        print(f"      Target: {target_name}")
        print(f"      Frequency: {schedule.frequency.value}")
        print(f"      Type: {schedule.backup_type.value}")
        print(f"      Next: {schedule.next_run.strftime('%Y-%m-%d %H:%M') if schedule.next_run else 'N/A'}")
        
    # Retention summary
    print("\nðŸ—‚ Retention Summary:")
    
    for policy in platform.retention_policies.values():
        print(f"  {policy.name}:")
        print(f"    Daily: keep {policy.daily_retention} backups")
        print(f"    Weekly: keep {policy.weekly_retention} backups")
        print(f"    Monthly: keep {policy.monthly_retention} backups")
        print(f"    Max age: {policy.max_age_days} days")
        
    # Statistics
    stats = platform.get_statistics()
    
    print("\nðŸ“ˆ Platform Statistics:")
    
    print(f"\n  Targets: {stats['total_targets']}")
    print(f"  Storages: {stats['total_storages']}")
    print(f"  Schedules: {stats['active_schedules']}")
    print(f"  Total Backups: {stats['total_backups']}")
    print(f"  Verified: {stats['verified_backups']}")
    print(f"  Total Size: {stats['total_size_gb']:.2f}GB")
    print(f"  Compressed: {stats['compressed_size_gb']:.2f}GB")
    print(f"  Compression: {stats['compression_ratio']:.1%}")
    
    # Dashboard
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                    Backup Management Dashboard                      â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Backup Targets:                {stats['total_targets']:>12}                        â”‚")
    print(f"â”‚ Active Schedules:              {stats['active_schedules']:>12}                        â”‚")
    print(f"â”‚ Total Backups:                 {stats['total_backups']:>12}                        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Verified Backups:              {stats['verified_backups']:>12}                        â”‚")
    print(f"â”‚ Total Storage:                   {stats['compressed_size_gb']:>10.2f}GB                â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\n" + "=" * 60)
    print("Backup Management Platform initialized!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
