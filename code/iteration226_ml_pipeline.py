#!/usr/bin/env python3
"""
Server Init - Iteration 226: ML Pipeline Platform
ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° ML Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð¾Ð²

Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»:
- Experiment Tracking - Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²
- Model Registry - Ñ€ÐµÐµÑÑ‚Ñ€ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
- Feature Store - Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ñ„Ð¸Ñ‡ÐµÐ¹
- Training Pipelines - Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ñ‹ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
- Model Deployment - Ð´ÐµÐ¿Ð»Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
- A/B Testing - A/B Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
- Model Monitoring - Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
- Data Versioning - Ð²ÐµÑ€ÑÐ¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
"""

import asyncio
import random
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from enum import Enum
import uuid


class ModelStatus(Enum):
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
    DRAFT = "draft"
    TRAINING = "training"
    TRAINED = "trained"
    VALIDATED = "validated"
    STAGING = "staging"
    PRODUCTION = "production"
    DEPRECATED = "deprecated"


class ModelFramework(Enum):
    """Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
    PYTORCH = "pytorch"
    TENSORFLOW = "tensorflow"
    SKLEARN = "sklearn"
    XGBOOST = "xgboost"
    LIGHTGBM = "lightgbm"
    ONNX = "onnx"


class ExperimentStatus(Enum):
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class FeatureType(Enum):
    """Ð¢Ð¸Ð¿ Ñ„Ð¸Ñ‡Ð¸"""
    NUMERICAL = "numerical"
    CATEGORICAL = "categorical"
    EMBEDDING = "embedding"
    TEXT = "text"
    IMAGE = "image"


@dataclass
class DatasetVersion:
    """Ð’ÐµÑ€ÑÐ¸Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
    version_id: str
    name: str = ""
    path: str = ""
    num_rows: int = 0
    num_features: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    hash: str = ""


@dataclass
class Feature:
    """Ð¤Ð¸Ñ‡Ð°"""
    feature_id: str
    name: str = ""
    feature_type: FeatureType = FeatureType.NUMERICAL
    description: str = ""
    entity: str = ""  # user, item, etc.
    source: str = ""  # table or transformation
    is_online: bool = False
    ttl_seconds: int = 3600


@dataclass
class Experiment:
    """Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚"""
    experiment_id: str
    name: str = ""
    description: str = ""
    
    # Parameters
    parameters: Dict[str, Any] = field(default_factory=dict)
    
    # Metrics
    metrics: Dict[str, float] = field(default_factory=dict)
    
    # Status
    status: ExperimentStatus = ExperimentStatus.PENDING
    
    # Dataset
    dataset_version: str = ""
    
    # Artifacts
    artifacts: List[str] = field(default_factory=list)
    
    # Times
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    duration_seconds: float = 0
    
    # Tags
    tags: List[str] = field(default_factory=list)


@dataclass
class ModelVersion:
    """Ð’ÐµÑ€ÑÐ¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
    version_id: str
    model_id: str = ""
    version: str = "1.0.0"
    
    # Framework
    framework: ModelFramework = ModelFramework.PYTORCH
    
    # Experiment
    experiment_id: str = ""
    
    # Artifacts
    model_path: str = ""
    model_size_mb: float = 0
    
    # Metrics
    metrics: Dict[str, float] = field(default_factory=dict)
    
    # Status
    status: ModelStatus = ModelStatus.DRAFT
    
    # Times
    created_at: datetime = field(default_factory=datetime.now)
    deployed_at: Optional[datetime] = None


@dataclass
class Model:
    """ÐœÐ¾Ð´ÐµÐ»ÑŒ"""
    model_id: str
    name: str = ""
    description: str = ""
    
    # Owner
    owner: str = ""
    team: str = ""
    
    # Versions
    versions: List[ModelVersion] = field(default_factory=list)
    current_version: str = ""
    
    # Tags
    tags: List[str] = field(default_factory=list)
    
    # Dates
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class Deployment:
    """Ð”ÐµÐ¿Ð»Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
    deployment_id: str
    model_id: str = ""
    version_id: str = ""
    
    # Endpoint
    endpoint_name: str = ""
    endpoint_url: str = ""
    
    # Resources
    replicas: int = 1
    cpu_cores: float = 1.0
    memory_gb: float = 2.0
    gpu_count: int = 0
    
    # Traffic
    traffic_percent: int = 100
    
    # Status
    is_active: bool = True
    
    # Times
    deployed_at: datetime = field(default_factory=datetime.now)


@dataclass
class ModelMetrics:
    """ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐµÐ½Ðµ"""
    metrics_id: str
    deployment_id: str = ""
    
    # Performance
    requests_per_second: float = 0
    latency_p50_ms: float = 0
    latency_p99_ms: float = 0
    
    # Quality
    prediction_drift: float = 0  # 0-1
    data_drift: float = 0  # 0-1
    
    # Collected at
    collected_at: datetime = field(default_factory=datetime.now)


class FeatureStore:
    """Ð¥Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ñ„Ð¸Ñ‡ÐµÐ¹"""
    
    def __init__(self):
        self.features: Dict[str, Feature] = {}
        self.feature_values: Dict[str, Dict[str, Any]] = {}  # entity_id -> feature values
        
    def create_feature(self, name: str, feature_type: FeatureType,
                      entity: str, source: str = "",
                      is_online: bool = False) -> Feature:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ„Ð¸Ñ‡Ð¸"""
        feature = Feature(
            feature_id=f"feat_{uuid.uuid4().hex[:8]}",
            name=name,
            feature_type=feature_type,
            entity=entity,
            source=source,
            is_online=is_online
        )
        self.features[feature.feature_id] = feature
        return feature
        
    def get_online_features(self, entity_id: str,
                           feature_names: List[str]) -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð½Ð»Ð°Ð¹Ð½ Ñ„Ð¸Ñ‡ÐµÐ¹"""
        result = {}
        values = self.feature_values.get(entity_id, {})
        for name in feature_names:
            result[name] = values.get(name)
        return result


class ExperimentTracker:
    """Ð¢Ñ€ÐµÐºÐµÑ€ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²"""
    
    def __init__(self):
        self.experiments: Dict[str, Experiment] = {}
        
    def create_experiment(self, name: str, parameters: Dict[str, Any],
                         dataset_version: str = "") -> Experiment:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°"""
        experiment = Experiment(
            experiment_id=f"exp_{uuid.uuid4().hex[:8]}",
            name=name,
            parameters=parameters,
            dataset_version=dataset_version
        )
        self.experiments[experiment.experiment_id] = experiment
        return experiment
        
    def start(self, experiment_id: str) -> bool:
        """Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°"""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return False
        exp.status = ExperimentStatus.RUNNING
        exp.started_at = datetime.now()
        return True
        
    def log_metrics(self, experiment_id: str, metrics: Dict[str, float]) -> bool:
        """Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº"""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return False
        exp.metrics.update(metrics)
        return True
        
    def complete(self, experiment_id: str) -> bool:
        """Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°"""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return False
        exp.status = ExperimentStatus.COMPLETED
        exp.completed_at = datetime.now()
        if exp.started_at:
            exp.duration_seconds = (exp.completed_at - exp.started_at).total_seconds()
        return True


class ModelRegistry:
    """Ð ÐµÐµÑÑ‚Ñ€ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    
    def __init__(self):
        self.models: Dict[str, Model] = {}
        self.versions: Dict[str, ModelVersion] = {}
        
    def register_model(self, name: str, owner: str, team: str) -> Model:
        """Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        model = Model(
            model_id=f"model_{uuid.uuid4().hex[:8]}",
            name=name,
            owner=owner,
            team=team
        )
        self.models[model.model_id] = model
        return model
        
    def create_version(self, model_id: str, version: str,
                      framework: ModelFramework, experiment_id: str,
                      metrics: Dict[str, float]) -> Optional[ModelVersion]:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸"""
        model = self.models.get(model_id)
        if not model:
            return None
            
        version_obj = ModelVersion(
            version_id=f"ver_{uuid.uuid4().hex[:8]}",
            model_id=model_id,
            version=version,
            framework=framework,
            experiment_id=experiment_id,
            metrics=metrics,
            model_size_mb=random.uniform(10, 500)
        )
        
        model.versions.append(version_obj)
        self.versions[version_obj.version_id] = version_obj
        return version_obj
        
    def promote_to_staging(self, version_id: str) -> bool:
        """ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð² staging"""
        version = self.versions.get(version_id)
        if not version:
            return False
        version.status = ModelStatus.STAGING
        return True
        
    def promote_to_production(self, version_id: str) -> bool:
        """ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð² production"""
        version = self.versions.get(version_id)
        if not version:
            return False
        version.status = ModelStatus.PRODUCTION
        version.deployed_at = datetime.now()
        
        # Update current version
        model = self.models.get(version.model_id)
        if model:
            model.current_version = version_id
        return True


class MLPipelinePlatform:
    """ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° ML Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð¾Ð²"""
    
    def __init__(self):
        self.feature_store = FeatureStore()
        self.experiments = ExperimentTracker()
        self.registry = ModelRegistry()
        self.datasets: Dict[str, DatasetVersion] = {}
        self.deployments: Dict[str, Deployment] = {}
        self.model_metrics: List[ModelMetrics] = []
        
    def create_dataset(self, name: str, path: str,
                      num_rows: int, num_features: int) -> DatasetVersion:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
        dataset = DatasetVersion(
            version_id=f"ds_{uuid.uuid4().hex[:8]}",
            name=name,
            path=path,
            num_rows=num_rows,
            num_features=num_features,
            hash=uuid.uuid4().hex[:16]
        )
        self.datasets[dataset.version_id] = dataset
        return dataset
        
    def run_experiment(self, name: str, params: Dict[str, Any],
                      dataset_id: str) -> Experiment:
        """Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°"""
        exp = self.experiments.create_experiment(name, params, dataset_id)
        self.experiments.start(exp.experiment_id)
        
        # Simulate training
        metrics = {
            "accuracy": random.uniform(0.85, 0.99),
            "precision": random.uniform(0.80, 0.98),
            "recall": random.uniform(0.80, 0.98),
            "f1_score": random.uniform(0.82, 0.97),
            "auc_roc": random.uniform(0.85, 0.99),
            "loss": random.uniform(0.01, 0.3)
        }
        
        self.experiments.log_metrics(exp.experiment_id, metrics)
        self.experiments.complete(exp.experiment_id)
        
        return exp
        
    def deploy_model(self, model_id: str, version_id: str,
                    endpoint_name: str, replicas: int = 1) -> Deployment:
        """Ð”ÐµÐ¿Ð»Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        deployment = Deployment(
            deployment_id=f"dep_{uuid.uuid4().hex[:8]}",
            model_id=model_id,
            version_id=version_id,
            endpoint_name=endpoint_name,
            endpoint_url=f"https://ml.api.example.com/{endpoint_name}",
            replicas=replicas
        )
        
        self.deployments[deployment.deployment_id] = deployment
        
        # Update version status
        version = self.registry.versions.get(version_id)
        if version:
            version.status = ModelStatus.PRODUCTION
            version.deployed_at = datetime.now()
            
        return deployment
        
    def collect_metrics(self, deployment_id: str) -> ModelMetrics:
        """Ð¡Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ñ€Ð¸Ðº"""
        metrics = ModelMetrics(
            metrics_id=f"met_{uuid.uuid4().hex[:8]}",
            deployment_id=deployment_id,
            requests_per_second=random.uniform(100, 1000),
            latency_p50_ms=random.uniform(10, 50),
            latency_p99_ms=random.uniform(50, 200),
            prediction_drift=random.uniform(0, 0.1),
            data_drift=random.uniform(0, 0.15)
        )
        self.model_metrics.append(metrics)
        return metrics
        
    def get_statistics(self) -> Dict[str, Any]:
        """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°"""
        experiments = list(self.experiments.experiments.values())
        completed = [e for e in experiments if e.status == ExperimentStatus.COMPLETED]
        
        models = list(self.registry.models.values())
        deployed = [d for d in self.deployments.values() if d.is_active]
        
        return {
            "total_datasets": len(self.datasets),
            "total_features": len(self.feature_store.features),
            "total_experiments": len(experiments),
            "completed_experiments": len(completed),
            "total_models": len(models),
            "total_versions": len(self.registry.versions),
            "active_deployments": len(deployed),
            "metrics_collected": len(self.model_metrics)
        }


# Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ
async def main():
    print("=" * 60)
    print("Server Init - Iteration 226: ML Pipeline Platform")
    print("=" * 60)
    
    platform = MLPipelinePlatform()
    print("âœ“ ML Pipeline Platform created")
    
    # Create datasets
    print("\nðŸ“Š Creating Datasets...")
    
    datasets = [
        platform.create_dataset("user_features_v1", "s3://ml-data/users", 1000000, 50),
        platform.create_dataset("transaction_data_v1", "s3://ml-data/transactions", 5000000, 75),
        platform.create_dataset("product_embeddings_v1", "s3://ml-data/products", 100000, 128),
    ]
    
    for ds in datasets:
        print(f"  âœ“ {ds.name}: {ds.num_rows:,} rows, {ds.num_features} features")
        
    # Create features
    print("\nðŸ”§ Creating Features...")
    
    features_config = [
        ("user_age", FeatureType.NUMERICAL, "user"),
        ("user_total_spend", FeatureType.NUMERICAL, "user"),
        ("user_segment", FeatureType.CATEGORICAL, "user"),
        ("item_price", FeatureType.NUMERICAL, "item"),
        ("item_category", FeatureType.CATEGORICAL, "item"),
        ("item_embedding", FeatureType.EMBEDDING, "item"),
        ("user_history_embedding", FeatureType.EMBEDDING, "user"),
    ]
    
    for name, ftype, entity in features_config:
        feature = platform.feature_store.create_feature(name, ftype, entity, "", True)
        print(f"  âœ“ {name}: {ftype.value} ({entity})")
        
    # Register models
    print("\nðŸ“¦ Registering Models...")
    
    models_config = [
        ("fraud_detector", "ML Team", "Risk"),
        ("recommendation_engine", "ML Team", "Recommendations"),
        ("churn_predictor", "ML Team", "Growth"),
        ("price_optimizer", "ML Team", "Pricing"),
    ]
    
    models = []
    for name, owner, team in models_config:
        model = platform.registry.register_model(name, owner, team)
        model.tags = [team.lower(), "production"]
        models.append(model)
        print(f"  âœ“ {name} ({team})")
        
    # Run experiments
    print("\nðŸ”¬ Running Experiments...")
    
    experiments = []
    for model in models:
        for i in range(2):
            params = {
                "learning_rate": random.choice([0.001, 0.01, 0.1]),
                "batch_size": random.choice([32, 64, 128]),
                "epochs": random.choice([10, 20, 50]),
                "hidden_layers": random.choice([[64, 32], [128, 64], [256, 128, 64]])
            }
            
            exp = platform.run_experiment(
                f"{model.name}_exp_{i+1}",
                params,
                datasets[0].version_id
            )
            experiments.append(exp)
            
    print(f"  âœ“ Completed {len(experiments)} experiments")
    
    # Create model versions
    print("\nðŸ“ Creating Model Versions...")
    
    frameworks = [ModelFramework.PYTORCH, ModelFramework.TENSORFLOW,
                 ModelFramework.SKLEARN, ModelFramework.XGBOOST]
    
    versions = []
    for i, model in enumerate(models):
        exp = experiments[i * 2]  # Best experiment for each model
        framework = frameworks[i % len(frameworks)]
        
        version = platform.registry.create_version(
            model.model_id,
            "1.0.0",
            framework,
            exp.experiment_id,
            exp.metrics
        )
        versions.append(version)
        print(f"  âœ“ {model.name} v1.0.0 ({framework.value})")
        
    # Promote to staging and production
    print("\nðŸš€ Promoting Models...")
    
    for version in versions:
        platform.registry.promote_to_staging(version.version_id)
        
    for version in versions[:3]:
        platform.registry.promote_to_production(version.version_id)
        print(f"  âœ“ {platform.registry.models[version.model_id].name} -> production")
        
    # Deploy models
    print("\nðŸŒ Deploying Models...")
    
    deployments = []
    for i, version in enumerate(versions[:3]):
        model = platform.registry.models[version.model_id]
        deployment = platform.deploy_model(
            model.model_id,
            version.version_id,
            f"{model.name}-endpoint",
            replicas=2
        )
        deployments.append(deployment)
        print(f"  âœ“ {model.name}: {deployment.endpoint_url}")
        
    # Collect metrics
    print("\nðŸ“ˆ Collecting Model Metrics...")
    
    for deployment in deployments:
        for _ in range(3):
            metrics = platform.collect_metrics(deployment.deployment_id)
            
    print(f"  âœ“ Collected metrics for {len(deployments)} deployments")
    
    # Display experiments
    print("\nðŸ”¬ Experiment Results:")
    
    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Experiment                  â”‚ Accuracy â”‚ F1 Score  â”‚ Duration â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for exp in experiments[:8]:
        name = exp.name[:27].ljust(27)
        acc = f"{exp.metrics.get('accuracy', 0)*100:.1f}%"[:8].ljust(8)
        f1 = f"{exp.metrics.get('f1_score', 0)*100:.1f}%"[:9].ljust(9)
        dur = f"{exp.duration_seconds:.0f}s"[:8].ljust(8)
        
        print(f"  â”‚ {name} â”‚ {acc} â”‚ {f1} â”‚ {dur} â”‚")
        
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Display models
    print("\nðŸ“¦ Model Registry:")
    
    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Model                  â”‚ Framework  â”‚ Status       â”‚ Accuracy â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for model in platform.registry.models.values():
        name = model.name[:20].ljust(20)
        
        if model.versions:
            latest = model.versions[-1]
            fw = latest.framework.value[:10].ljust(10)
            status = latest.status.value[:12].ljust(12)
            acc = f"{latest.metrics.get('accuracy', 0)*100:.1f}%"[:8].ljust(8)
        else:
            fw = "N/A".ljust(10)
            status = "no version".ljust(12)
            acc = "N/A".ljust(8)
            
        print(f"  â”‚ {name} â”‚ {fw} â”‚ {status} â”‚ {acc} â”‚")
        
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Active deployments
    print("\nðŸŒ Active Deployments:")
    
    for dep in deployments:
        model = platform.registry.models.get(dep.model_id)
        if model:
            print(f"  â€¢ {model.name}")
            print(f"    URL: {dep.endpoint_url}")
            print(f"    Replicas: {dep.replicas}")
            
    # Model metrics
    print("\nðŸ“Š Model Performance:")
    
    for dep in deployments:
        model = platform.registry.models.get(dep.model_id)
        if model:
            metrics = [m for m in platform.model_metrics if m.deployment_id == dep.deployment_id]
            if metrics:
                latest = metrics[-1]
                print(f"  {model.name}:")
                print(f"    RPS: {latest.requests_per_second:.0f}")
                print(f"    Latency p50: {latest.latency_p50_ms:.0f}ms")
                print(f"    Drift: {latest.prediction_drift*100:.1f}%")
                
    # Feature store summary
    print("\nðŸ”§ Feature Store:")
    
    by_type = {}
    for f in platform.feature_store.features.values():
        t = f.feature_type.value
        if t not in by_type:
            by_type[t] = 0
        by_type[t] += 1
        
    for ftype, count in by_type.items():
        bar = "â–ˆ" * count + "â–‘" * (5 - count)
        print(f"  {ftype:12s} [{bar}] {count}")
        
    # Statistics
    print("\nðŸ“ˆ Platform Statistics:")
    
    stats = platform.get_statistics()
    
    print(f"\n  Datasets: {stats['total_datasets']}")
    print(f"  Features: {stats['total_features']}")
    print(f"  Experiments: {stats['total_experiments']} ({stats['completed_experiments']} completed)")
    print(f"  Models: {stats['total_models']}")
    print(f"  Versions: {stats['total_versions']}")
    print(f"  Deployments: {stats['active_deployments']}")
    
    # Dashboard
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                       ML Pipeline Dashboard                         â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Total Models:                  {stats['total_models']:>12}                        â”‚")
    print(f"â”‚ Model Versions:                {stats['total_versions']:>12}                        â”‚")
    print(f"â”‚ Active Deployments:            {stats['active_deployments']:>12}                        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Experiments Run:               {stats['total_experiments']:>12}                        â”‚")
    print(f"â”‚ Features Available:            {stats['total_features']:>12}                        â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\n" + "=" * 60)
    print("ML Pipeline Platform initialized!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
